<div class="comparison-content">
    <h2>Comparison: Three Regimes of Reputation with Markov States</h2>
    
    <h3>Framework Comparison</h3>
    
    <div class="comparison-grid">
        <div class="comparison-card">
            <h4>i.i.d. (Luo &amp; Wolitzky)</h4>
            <div class="equation-block">
                $$\theta_t \sim \pi(\cdot) \quad \text{i.i.d.}$$
            </div>
            <p><strong>State space:</strong> \(\Theta\)</p>
            <p><strong>Strategy:</strong> \(s_1: \Theta \to \Delta(A_1)\)</p>
            <p><strong>SR belief:</strong> Always \(\pi\)</p>
            <p><strong>Payoff bound:</strong> \(V(s_1^*)\)</p>
        </div>
        <div class="comparison-card">
            <h4>Markovian Extension</h4>
            <div class="equation-block">
                $$\theta_t \sim F(\cdot \mid \theta_{t-1})$$
            </div>
            <p><strong>State space:</strong> \(\tilde\Theta = \Theta \times \Theta\)</p>
            <p><strong>Strategy:</strong> \(s_1: \tilde\Theta \to \Delta(A_1)\)</p>
            <p><strong>SR belief:</strong> \(F(\cdot|\theta_t) \neq \pi\)</p>
            <p><strong>Payoff bound:</strong> depends on belief-robustness</p>
        </div>
    </div>

    <h3>The Three Regimes</h3>

    <table>
        <thead>
            <tr>
                <th>Regime</th>
                <th>SR Belief About \(\theta_{t+1}\)</th>
                <th>SR Behavior</th>
                <th>Payoff Bound</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>i.i.d.</strong></td>
                <td>\(\pi\) (stationary)</td>
                <td>Static</td>
                <td>\(V(s_1^*)\)</td>
            </tr>
            <tr style="background: rgba(46, 204, 113, 0.08);">
                <td><strong>Markov (belief-robust)</strong></td>
                <td>\(F(\cdot|\theta_t)\) but irrelevant</td>
                <td>Static (same \(\forall \theta\))</td>
                <td>\(V(s_1^*)\) (same!)</td>
            </tr>
            <tr style="background: rgba(231, 76, 60, 0.08);">
                <td><strong>Markov (general)</strong></td>
                <td>\(F(\cdot|\theta_t)\) and relevant</td>
                <td>State-contingent</td>
                <td>\(V_{\mathrm{Markov}} \leq V(s_1^*)\)</td>
            </tr>
        </tbody>
    </table>

    <h3>Proof Step Comparison</h3>
    
    <table>
        <thead>
            <tr>
                <th>Component</th>
                <th>i.i.d. (Luo &amp; Wolitzky)</th>
                <th>Markov (This Paper)</th>
                <th>Change?</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>OT Problem</strong></td>
                <td>\(\text{OT}(\rho, \phi; \alpha_2)\) on \(Y_0 \times A_1\)</td>
                <td>\(\text{OT}(\tilde\rho, \phi; \alpha_2)\) on \(\tilde\Theta \times A_1\)</td>
                <td>Notation only</td>
            </tr>
            <tr>
                <td><strong>Cyclical Monotonicity</strong></td>
                <td>On \(\Theta \times A_1\)</td>
                <td>On \(\tilde\Theta \times A_1\) (first-coord order)</td>
                <td>Expanded space</td>
            </tr>
            <tr>
                <td><strong>KL Bound</strong></td>
                <td>\(\bar{T} = -2\log\mu_0/\eta^2\)</td>
                <td>\(\bar{T} = -2\log\mu_0/\eta^2\)</td>
                <td><strong>No change!</strong></td>
            </tr>
            <tr>
                <td><strong>Posterior Convergence</strong></td>
                <td>Martingale convergence</td>
                <td>Martingale + filter stability</td>
                <td>+Ergodicity</td>
            </tr>
            <tr style="background: rgba(231,76,60,0.08);">
                <td><strong>Equilibrium (Step 1)</strong></td>
                <td>Continuation value independent of \(\theta_t\)</td>
                <td>Continuation value depends on \(\theta_t\)</td>
                <td><strong>New: supermodularity absorbs</strong></td>
            </tr>
            <tr style="background: rgba(231,76,60,0.08);">
                <td><strong>Payoff Bound (Step 5)</strong></td>
                <td>\(V(s_1^*)\) &mdash; SR behavior static</td>
                <td>Belief-robust: \(V(s_1^*)\)<br>General: \(V_{\mathrm{Markov}} \leq V(s_1^*)\)</td>
                <td><strong>New: belief-robustness splits cases</strong></td>
            </tr>
        </tbody>
    </table>

    <h3>The Cost of Persistence</h3>

    <div class="result-box" style="border-left: 4px solid #e67e22;">
        <div class="box-title" style="color: #e67e22;">New Economic Object</div>
        <p>The gap \(V(s_1^*) - V_{\mathrm{Markov}}\) is the <strong>cost of persistence in reputation games</strong>. It quantifies exactly how much the long-run player loses because state persistence enables short-run players to condition behavior on the revealed state.</p>
        <div class="equation-block">
            $$\text{Cost of persistence} = V(s_1^*) - V_{\mathrm{Markov}}(s_1^*) = \sum_\theta \pi(\theta)\left[\inf_{B(s_1^*,\pi)} u_1 - \inf_{B(s_1^*, F(\cdot|\theta))} u_1\right]$$
        </div>
        <p>Properties:</p>
        <ul>
            <li>Equals <strong>zero</strong> if and only if the game is belief-robust</li>
            <li><strong>Vanishes continuously</strong> as \(\alpha + \beta \to 1\) (the i.i.d. limit)</li>
            <li><strong>Increasing</strong> in the persistence parameter \(|1-\alpha-\beta|\)</li>
        </ul>
    </div>

    <h3>Interactive Comparison: Deterrence Game</h3>
    
    <p>Compare the commitment payoff across persistence levels. Note: when the Markov and i.i.d. cases share the same \(\pi(G)\), the <em>stationary</em> commitment payoff is identical &mdash; but the <em>realized</em> payoff may differ if belief-robustness fails.</p>

    <div class="controls">
        <h4>Comparison Parameters</h4>
        
        <div class="control-group">
            <label>i.i.d. prob. of G: <span class="slider-value" id="comp-pi-value">0.625</span></label>
            <input type="range" id="comp-pi-slider" min="0.1" max="0.9" step="0.025" value="0.625">
        </div>

        <div class="control-group">
            <label>Markov \(\alpha\) (G&rarr;B): <span class="slider-value" id="comp-alpha-value">0.30</span></label>
            <input type="range" id="comp-alpha-slider" min="0.05" max="0.95" step="0.05" value="0.3">
        </div>

        <div class="control-group">
            <label>Markov \(\beta\) (B&rarr;G): <span class="slider-value" id="comp-beta-value">0.50</span></label>
            <input type="range" id="comp-beta-slider" min="0.05" max="0.95" step="0.05" value="0.5">
        </div>
    </div>

    <div class="result-box">
        <div class="box-title">Commitment Payoffs</div>
        <table>
            <thead>
                <tr>
                    <th>Case</th>
                    <th>Stationary \(\pi(G)\)</th>
                    <th>\(V(s_1^*)\)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>i.i.d.</strong></td>
                    <td id="comp-iid-pi">0.625</td>
                    <td id="comp-iid-v">0.625</td>
                </tr>
                <tr>
                    <td><strong>Markovian</strong></td>
                    <td id="comp-markov-pi">0.625</td>
                    <td id="comp-markov-v">0.625</td>
                </tr>
                <tr>
                    <td><strong>Difference</strong></td>
                    <td id="comp-diff-pi">0.000</td>
                    <td id="comp-diff-v">0.000</td>
                </tr>
            </tbody>
        </table>
        <p style="margin-top: 0.5rem; font-size: 0.9rem; color: #666;"><em>Note: These values show the stationary commitment payoff \(V(s_1^*)\). The actual bound for non-belief-robust games is \(V_{\mathrm{Markov}} \leq V(s_1^*)\). See the Deterrence Example tab for the full calculation with belief-robustness.</em></p>
    </div>

    <div class="plot-container">
        <div id="comparison-payoff-plot"></div>
    </div>

    <h3>Limiting Cases</h3>
    
    <p>The Markovian framework interpolates between two extremes:</p>

    <div class="comparison-grid">
        <div class="comparison-card">
            <h4>Fast Mixing (&rarr; i.i.d.)</h4>
            <p><strong>Regime:</strong> \(\alpha + \beta \to 1\)</p>
            <p>\(F(\cdot|\theta) \approx \pi\)</p>
            <p>Belief-robustness holds generically</p>
            <p>\(V_{\mathrm{Markov}} = V(s_1^*)\)</p>
            <p><strong>Recovers Luo &amp; Wolitzky (2024)</strong></p>
        </div>
        <div class="comparison-card">
            <h4>Slow Mixing (&rarr; Pei 2020)</h4>
            <p><strong>Regime:</strong> \(\alpha, \beta \to 0\)</p>
            <p>\(F(\cdot|\theta) \to \delta_\theta\)</p>
            <p>Belief-robustness fails generically</p>
            <p>Cost of persistence maximized</p>
            <p><strong>Framework degrades; Pei's approach needed</strong></p>
        </div>
    </div>

    <div class="plot-container">
        <div id="mixing-comparison-plot"></div>
    </div>

    <h3>Economic Insights</h3>
    
    <div class="extension-box">
        <div class="box-title">New Economic Content from the Markov Framework</div>
        <ul>
            <li><strong>Temporal patterns as identification:</strong> Autocorrelation in actions provides an additional channel beyond marginal frequencies, making confound-defeating conditions easier to satisfy in the supermodular case.</li>
            <li><strong>Transition-contingent commitment types:</strong> The lifted state allows types that condition on state <em>transitions</em> (e.g., "fight only when state deteriorates") &mdash; natural in dynamic settings with no i.i.d. counterpart.</li>
            <li><strong>Persistence is not uniformly harmful:</strong> Under belief-robustness, the payoff bound is identical to i.i.d. The cost of persistence arises <em>only</em> when \(\mu^*\) falls in the danger zone \([\beta, 1-\alpha]\).</li>
            <li><strong>Regime-dependent reputation:</strong> The framework directly links transition rates to the value of reputation, providing new economic content about how dynamic environments affect long-run players.</li>
        </ul>
    </div>

    <h3>Summary Table</h3>
    
    <table>
        <thead>
            <tr>
                <th>Aspect</th>
                <th>i.i.d.</th>
                <th>Markov (belief-robust)</th>
                <th>Markov (general)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>State evolution</td>
                <td>Independent</td>
                <td colspan="2">Dependent (Markov chain)</td>
            </tr>
            <tr>
                <td>Effective state</td>
                <td>\(\theta_t\)</td>
                <td colspan="2">\(\tilde\theta_t = (\theta_t, \theta_{t-1})\)</td>
            </tr>
            <tr>
                <td>SR belief about \(\theta_{t+1}\)</td>
                <td>\(\pi\)</td>
                <td>\(F(\cdot|\theta_t)\) (irrelevant)</td>
                <td>\(F(\cdot|\theta_t)\) (relevant)</td>
            </tr>
            <tr>
                <td>SR behavior</td>
                <td>Static</td>
                <td>Static</td>
                <td>State-contingent</td>
            </tr>
            <tr>
                <td>KL bound</td>
                <td>\(-2\log\mu_0/\eta^2\)</td>
                <td colspan="2">\(-2\log\mu_0/\eta^2\) (same!)</td>
            </tr>
            <tr>
                <td>Commitment payoff</td>
                <td>\(V(s_1^*)\)</td>
                <td>\(V(s_1^*)\) (same!)</td>
                <td>\(V_{\mathrm{Markov}} \leq V(s_1^*)\)</td>
            </tr>
            <tr>
                <td>Cost of persistence</td>
                <td>0</td>
                <td>0</td>
                <td>\(V(s_1^*) - V_{\mathrm{Markov}} > 0\)</td>
            </tr>
        </tbody>
    </table>
</div>
