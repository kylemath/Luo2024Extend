<div class="original-content">
    <h2>The Luo &amp; Wolitzky (2024) Result: i.i.d. States</h2>
    
    <h3>Model Setup</h3>
    
    <p>In each period t, the state θ<sub>t</sub> is drawn <strong>independently and identically</strong> from a distribution π:</p>
    
    <div class="equation-block">
        $$\theta_t \sim \pi(\cdot) \quad \text{i.i.d. across } t$$
    </div>

    <p>The long-run player observes θ<sub>t</sub> privately and plays a strategy s₁: Θ → Δ(A₁). Short-run players observe only the marginal distribution over actions.</p>

    <h3>The Confound-Defeating Property</h3>
    
    <div class="theorem-box">
        <div class="box-title">Definition: Confound-Defeating</div>
        <p>A strategy s₁* is <strong>confound-defeating</strong> if for every best response (α₀, α₂) ∈ B₀(s₁*), the joint distribution γ(α₀, s₁*) is the <strong>unique solution</strong> to:</p>
        <div class="equation-block">
            $$\text{OT}(\rho(\alpha_0), \phi(\alpha_0, s_1^*); \alpha_2): \max_{\gamma} \int u_1(y_0, a_1, \alpha_2)\, d\gamma$$
        </div>
        <p>subject to: π<sub>Y₀</sub>(γ) = ρ(α₀) and π<sub>A₁</sub>(γ) = φ(α₀, s₁*)</p>
    </div>

    <h3>Characterization: Cyclical Monotonicity</h3>
    
    <p>The confound-defeating property is equivalent to <strong>strict cyclical monotonicity</strong> of supp(s₁*):</p>
    
    <div class="equation-block">
        <div class="equation-label">For any cycle {(y<sub>i</sub>, a<sub>i</sub>)}<sup>N</sup><sub>i=1</sub> ⊂ supp(s₁*):</div>
        $$\sum_{i=1}^N u_1(y_i, a_i, \alpha_2) > \sum_{i=1}^N u_1(y_i, a_{i+1}, \alpha_2)$$
    </div>

    <h3>Interactive Example: Deterrence Game</h3>
    
    <p>Consider a deterrence game where the long-run player observes whether an attack occurs (θ ∈ {G, B}) and decides whether to Fight (F) or Acquiesce (A).</p>

    <div class="controls">
        <h4>Parameters</h4>
        
        <div class="control-group">
            <label>Probability of Good state (π<sub>G</sub>): <span class="slider-value" id="pi-value">0.60</span></label>
            <input type="range" id="pi-slider" min="0.1" max="0.9" step="0.05" value="0.6">
        </div>

        <div class="control-group">
            <label>Payoff u₁(G, F) [x]: <span class="slider-value" id="x-value">0.30</span></label>
            <input type="range" id="x-slider" min="0" max="1" step="0.05" value="0.3">
        </div>

        <div class="control-group">
            <label>Payoff u₁(B, A) [y]: <span class="slider-value" id="y-value">0.40</span></label>
            <input type="range" id="y-slider" min="0" max="1" step="0.05" value="0.4">
        </div>
    </div>

    <div class="result-box">
        <div class="box-title">Payoff Structure</div>
        <table>
            <thead>
                <tr>
                    <th>State</th>
                    <th>Acquiesce (A)</th>
                    <th>Fight (F)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Good (G)</strong></td>
                    <td>1</td>
                    <td id="payoff-gf">x = 0.30</td>
                </tr>
                <tr>
                    <td><strong>Bad (B)</strong></td>
                    <td id="payoff-ba">y = 0.40</td>
                    <td>0</td>
                </tr>
            </tbody>
        </table>
    </div>

    <div class="plot-container">
        <div id="commitment-payoff-plot"></div>
    </div>

    <div id="supermodular-result" class="extension-box">
        <div class="box-title">Result</div>
        <p id="result-text"></p>
    </div>

    <h3>The KL-Divergence Counting Bound</h3>
    
    <p>A key step in the proof is bounding the number of "distinguishing periods" where short-run players can tell the difference between the Stackelberg strategy and the equilibrium strategy:</p>

    <div class="equation-block">
        <div class="equation-label">Lemma 2 (KL Bound):</div>
        $$\bar{T}(\eta, \mu_0) = \frac{-2\log\mu_0(\omega_{s_1^*})}{\eta^2}$$
    </div>

    <div class="controls">
        <h4>KL Bound Parameters</h4>
        
        <div class="control-group">
            <label>Prior probability μ₀(ω<sub>s₁*</sub>): <span class="slider-value" id="mu0-value">0.01</span></label>
            <input type="range" id="mu0-slider" min="0.001" max="0.1" step="0.001" value="0.01">
        </div>

        <div class="control-group">
            <label>Tolerance η: <span class="slider-value" id="eta-value">0.10</span></label>
            <input type="range" id="eta-slider" min="0.01" max="0.3" step="0.01" value="0.1">
        </div>
    </div>

    <div class="plot-container">
        <div id="kl-bound-plot"></div>
    </div>

    <div class="result-box">
        <div class="box-title">Interpretation</div>
        <p>The bound <span id="kl-bound-value">921</span> periods means that, in expectation, there can be at most this many periods where the equilibrium behavior differs noticeably from the commitment strategy. This bound is derived using:</p>
        <ul>
            <li>KL divergence chain rule (valid for any stochastic process)</li>
            <li>Bayesian updating identity</li>
            <li>Pinsker's inequality</li>
        </ul>
    </div>

    <h3>Main Theorem</h3>
    
    <div class="theorem-box">
        <div class="box-title">Theorem 1 (Luo & Wolitzky 2024)</div>
        <p>Suppose s₁* is confound-defeating and not behaviorally confounded. Then:</p>
        <div class="equation-block">
            $$\liminf_{\delta \to 1} \underline{U}_1(\delta) \geq V(s_1^*)$$
        </div>
        <p>where V(s₁*) = inf<sub>(α₀,α₂) ∈ B(s₁*)</sub> u₁(α₀, s₁*, α₂) is the commitment payoff.</p>
    </div>

    <p><strong>Intuition:</strong> A patient long-run player can secure her commitment payoff by building a reputation for playing the Stackelberg strategy. The confound-defeating property ensures that short-run players who observe the Stackelberg action distribution must conclude the long-run player is playing the Stackelberg strategy — and best respond accordingly.</p>
</div>
