<div class="intro-content">
    <h2>Extending Marginal Reputation to Persistent Markovian States</h2>
    
    <div class="result-box">
        <div class="box-title">The $500 Challenge (Feb 16, 2026)</div>
        <p><strong>Daniel Luo (MIT):</strong> <em>"I will pay you $500 if you can figure out how to extend the main result to allow for persistent/markovian states (something we suspect is possible but never did) in &lt;5 hours of time."</em></p>
    </div>

    <h3>The Original Result (Luo &amp; Wolitzky, 2024)</h3>
    
    <p><strong>"Marginal Reputation"</strong> by Luo &amp; Wolitzky (2024) establishes a connection between reputation theory in repeated games and optimal transport theory. Their main result shows that a patient long-run player can secure her <em>commitment payoff</em> \(V(s_1^*)\) if her Stackelberg strategy is <em>confound-defeating</em> and <em>not behaviorally confounded</em>. Throughout, states are drawn <strong>i.i.d. across periods</strong>. The authors note (footnote 9) that the extension to persistent states is an open question.</p>

    <div class="theorem-box">
        <div class="box-title">Theorem 1 (Luo &amp; Wolitzky, 2024 &mdash; i.i.d. case)</div>
        <p>If the Stackelberg strategy \(s_1^*\) is confound-defeating and not behaviorally confounded, then:</p>
        <div class="equation-block">
            $$\liminf_{\delta \to 1} \underline{U}_1(\delta) \geq V(s_1^*)$$
        </div>
    </div>

    <h3>The Answer: Partially Yes, Partially No</h3>

    <p>The extension from i.i.d. to Markov states is <strong>not</strong> a straightforward substitution. The key new phenomenon: when the Stackelberg strategy reveals the state (e.g., \(s_1^*(G)=A, s_1^*(B)=F\)), short-run players learn \(\theta_t\) and form beliefs about \(\theta_{t+1}\) using the <em>filtering distribution</em> \(F(\cdot|\theta_t)\) rather than the stationary distribution \(\pi\). This makes short-run behavior <strong>state-contingent</strong>.</p>

    <div class="extension-box">
        <div class="box-title">The Key New Concept: Belief-Robustness</div>
        <p>A game is <strong>belief-robust</strong> if the short-run player's best-response set does not depend on the revealed state:</p>
        <div class="equation-block">
            $$B(s_1^*, F(\cdot|\theta)) = B(s_1^*, F(\cdot|\theta')) \quad \text{for all } \theta, \theta'$$
        </div>
        <p>For the deterrence game, belief-robustness holds iff the SR threshold \(\mu^*\) lies <strong>outside</strong> the "danger zone" \([\beta, 1-\alpha]\).</p>
    </div>

    <h3>Two Corrected Theorems</h3>

    <div class="comparison-grid">
        <div class="comparison-card">
            <h4>Theorem 1' (Belief-Robust)</h4>
            <p>When the game is belief-robust, the full i.i.d. bound holds exactly:</p>
            <div class="equation-block">
                $$\liminf_{\delta \to 1} \underline{U}_1(\delta) \geq V(s_1^*)$$
            </div>
            <p>All proof machinery transfers without modification.</p>
        </div>
        <div class="comparison-card">
            <h4>Theorem 1'' (General Case)</h4>
            <p>For all supermodular games, a corrected bound holds:</p>
            <div class="equation-block">
                $$\liminf_{\delta \to 1} \underline{U}_1(\delta) \geq V_{\mathrm{Markov}}(s_1^*) \leq V(s_1^*)$$
            </div>
            <p>Equality iff belief-robust. The gap is the <strong>"cost of persistence."</strong></p>
        </div>
    </div>

    <div class="result-box">
        <div class="box-title">The Cost of Persistence</div>
        <p>The gap \(V(s_1^*) - V_{\mathrm{Markov}}\) is a new economic object quantifying how much the long-run player loses because state persistence enables short-run players to condition behavior on the revealed state. It vanishes continuously as the chain approaches the i.i.d. limit (\(\alpha + \beta \to 1\)).</p>
    </div>

    <h3>What Extends and What Doesn't</h3>

    <div class="comparison-grid">
        <div class="comparison-card" style="border-left: 3px solid #2ecc71;">
            <h4 style="color: #2ecc71;">Extends Unchanged</h4>
            <ul>
                <li><strong>KL counting bound</strong> &mdash; identical, no mixing-time correction</li>
                <li><strong>OT / confound-defeating</strong> &mdash; works on lifted space \(\tilde\Theta\)</li>
                <li><strong>Filter stability</strong> &mdash; exponential forgetting holds</li>
                <li><strong>Supermodularity / monotonicity</strong> &mdash; first-coordinate order suffices</li>
            </ul>
        </div>
        <div class="comparison-card" style="border-left: 3px solid #e74c3c;">
            <h4 style="color: #e74c3c;">Requires New Ideas</h4>
            <ul>
                <li><strong>SR belief dynamics</strong> &mdash; beliefs permanently deviate from \(\pi\)</li>
                <li><strong>Nash correspondence</strong> &mdash; becomes state-contingent \(B(s_1^*, F(\cdot|\theta))\)</li>
                <li><strong>Payoff bound</strong> &mdash; needs belief-robustness or corrected \(V_{\mathrm{Markov}}\)</li>
                <li><strong>Continuation value</strong> &mdash; depends on \(\theta_t\) through transition kernel</li>
            </ul>
        </div>
    </div>

    <h3>The Lifted State Construction</h3>

    <p>The mathematical framework still begins with the <em>lifted state</em> \(\tilde\theta_t = (\theta_t, \theta_{t-1})\), which provides a stationary distribution \(\tilde\rho\) on the expanded space and enables the optimal transport framework. But the lifting alone is <strong>not sufficient</strong> &mdash; the belief-robustness distinction is essential for the correct payoff bound.</p>

    <h3>Navigation Guide</h3>
    
    <table>
        <thead>
            <tr>
                <th>Tab</th>
                <th>Content</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Original (i.i.d.)</strong></td>
                <td>Interactive exploration of the Luo &amp; Wolitzky (2024) result with i.i.d. states</td>
            </tr>
            <tr>
                <td><strong>Markovian Extension</strong></td>
                <td>Lifted state construction, belief-robustness, and two corrected theorems</td>
            </tr>
            <tr>
                <td><strong>Comparison</strong></td>
                <td>Three-regime comparison: i.i.d. vs. belief-robust Markov vs. general Markov</td>
            </tr>
            <tr>
                <td><strong>Deterrence Example</strong></td>
                <td>Worked example with belief-robust and non-belief-robust versions</td>
            </tr>
            <tr>
                <td><strong>Author Review</strong></td>
                <td>Daniel Luo's 15-point critique with severity classifications</td>
            </tr>
            <tr>
                <td><strong>Revision</strong></td>
                <td>Point-by-point responses with interactive belief gap and payoff visualizations</td>
            </tr>
            <tr>
                <td><strong>Prompt History</strong></td>
                <td>Full AI agent conversation transcripts from both phases</td>
            </tr>
        </tbody>
    </table>

    <div class="result-box" style="text-align: center;">
        <div class="box-title">Papers</div>
        <p><a href="revisedTexPaper/main.pdf" style="font-size: 1.1rem; font-weight: bold; color: var(--secondary-color);">Revised Paper (v2, 28 pages)</a> &middot; <a href="revisedTexPaper/response_letter.pdf" style="font-size: 0.95rem; color: var(--secondary-color);">Response Letter</a></p>
        <p>Corrected theorems, belief-robustness framework, and computational verification addressing all 15 critiques from Daniel Luo.</p>
        <hr style="border: none; border-top: 1px solid rgba(255,255,255,0.2); margin: 1rem 0;">
        <p><a href="texPaper/marginal_reputation_markov_extension.pdf" style="font-size: 0.95rem; color: var(--secondary-color);">Initial Draft (v1, 26 pages)</a></p>
        <p style="font-size: 0.9rem;">Original AI-assisted conjecture produced under the 5-hour time constraint.</p>
    </div>

    <p class="text-center mt-2"><em>Explore the tabs above to dive into the mathematics and interactive visualizations.</em></p>
</div>
