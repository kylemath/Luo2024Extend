<div class="markov-content">
    <h2>Markovian Extension: Lifted States and Belief-Robustness</h2>
    
    <h3>From i.i.d. to Markov States</h3>
    
    <p>Luo &amp; Wolitzky (2024) assume states \(\theta_t\) are drawn <strong>i.i.d.</strong> across periods. We extend to the case where \(\theta_t\) follows a <strong>stationary ergodic Markov chain</strong>:</p>
    
    <div class="equation-block">
        $$\theta_t \sim F(\cdot \mid \theta_{t-1})$$
        <p style="margin-top: 0.5rem;">with unique stationary distribution \(\pi\) and transition kernel \(F\).</p>
    </div>

    <h3>Step 1: The Lifted State Construction</h3>
    
    <p>The state is redefined as the <strong>pair</strong>:</p>
    
    <div class="extension-box">
        <div class="box-title">Definition: Lifted State</div>
        <div class="equation-block">
            $$\tilde{\theta}_t = (\theta_t, \theta_{t-1}) \in \tilde{\Theta} = \Theta \times \Theta$$
        </div>
        <p>Under ergodicity, the lifted chain has a <strong>fixed stationary distribution</strong>:</p>
        <div class="equation-block">
            $$\tilde{\rho}(\theta, \theta') = \pi(\theta') \cdot F(\theta \mid \theta')$$
        </div>
        <p>This plays the role of the i.i.d. signal distribution in Luo &amp; Wolitzky (2024), enabling the optimal transport framework on the expanded space.</p>
    </div>

    <h3>Interactive Markov Chain Visualization</h3>
    
    <p>Consider a two-state chain \(\theta_t \in \{G, B\}\) with transition probabilities:</p>

    <div class="controls">
        <h4>Markov Chain Parameters</h4>
        
        <div class="control-group">
            <label>\(\alpha\) (G&rarr;B transition): <span class="slider-value" id="alpha-value">0.30</span></label>
            <input type="range" id="alpha-slider" min="0.05" max="0.95" step="0.05" value="0.3">
        </div>

        <div class="control-group">
            <label>\(\beta\) (B&rarr;G transition): <span class="slider-value" id="beta-value">0.50</span></label>
            <input type="range" id="beta-slider" min="0.05" max="0.95" step="0.05" value="0.5">
        </div>
    </div>

    <div class="comparison-grid">
        <div class="comparison-card">
            <h4>Transition Matrix</h4>
            <table>
                <thead>
                    <tr>
                        <th>From\To</th>
                        <th>G</th>
                        <th>B</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>G</strong></td>
                        <td id="trans-gg">0.70</td>
                        <td id="trans-gb">0.30</td>
                    </tr>
                    <tr>
                        <td><strong>B</strong></td>
                        <td id="trans-bg">0.50</td>
                        <td id="trans-bb">0.50</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div class="comparison-card">
            <h4>Stationary Distribution</h4>
            <div class="equation-block">
                $$\pi(G) = \frac{\beta}{\alpha + \beta}$$
            </div>
            <table>
                <tbody>
                    <tr>
                        <td><strong>\(\pi(G)\):</strong></td>
                        <td id="stat-g">0.625</td>
                    </tr>
                    <tr>
                        <td><strong>\(\pi(B)\):</strong></td>
                        <td id="stat-b">0.375</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="plot-container">
        <div id="markov-trajectory-plot"></div>
    </div>

    <h3>Lifted State Distribution</h3>
    
    <p>The lifted state \(\tilde\theta_t = (\theta_t, \theta_{t-1})\) lives in a 4-element space:</p>

    <div class="result-box">
        <div class="box-title">Lifted Stationary Distribution</div>
        <table id="lifted-dist-table">
            <thead>
                <tr>
                    <th>\(\tilde\theta = (\theta_t, \theta_{t-1})\)</th>
                    <th>\(\tilde\rho(\tilde\theta)\)</th>
                    <th>Formula</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>(G, G)</strong></td>
                    <td id="lifted-gg">0.4375</td>
                    <td>\(\pi(G) \times (1-\alpha)\)</td>
                </tr>
                <tr>
                    <td><strong>(G, B)</strong></td>
                    <td id="lifted-gb">0.1875</td>
                    <td>\(\pi(B) \times \beta\)</td>
                </tr>
                <tr>
                    <td><strong>(B, G)</strong></td>
                    <td id="lifted-bg">0.1875</td>
                    <td>\(\pi(G) \times \alpha\)</td>
                </tr>
                <tr>
                    <td><strong>(B, B)</strong></td>
                    <td id="lifted-bb">0.1875</td>
                    <td>\(\pi(B) \times (1-\beta)\)</td>
                </tr>
            </tbody>
        </table>
    </div>

    <div class="plot-container">
        <div id="lifted-distribution-plot"></div>
    </div>

    <h3>Step 2: The Fundamental New Phenomenon</h3>

    <div class="result-box" style="border-left: 4px solid #e74c3c;">
        <div class="box-title" style="color: #e74c3c;">The Filtering Belief Problem</div>
        <p>The lifted state construction is <strong>necessary but not sufficient</strong>. When the Stackelberg strategy reveals the state (e.g., \(s_1^*(G)=A, s_1^*(B)=F\)), the short-run player learns \(\theta_t\) exactly, and their belief about \(\theta_{t+1}\) becomes the <strong>filtering distribution</strong> \(F(\cdot|\theta_t)\), which <em>permanently differs</em> from the stationary distribution \(\pi\):</p>
        <div class="equation-block">
            $$F(G|G) = 1 - \alpha \neq \pi(G) = \frac{\beta}{\alpha + \beta} \neq \beta = F(G|B)$$
        </div>
        <p>This makes the short-run player's best response <strong>state-contingent</strong>: the Nash correspondence \(B(s_1^*)\) must be replaced by \(B(s_1^*, F(\cdot|\theta))\).</p>
    </div>

    <h3>Step 3: Belief-Robustness &mdash; The Key New Concept</h3>

    <div class="extension-box">
        <div class="box-title">Definition: Belief-Robustness</div>
        <p>A game is <strong>belief-robust</strong> if the SR best-response set is invariant to the revealed state:</p>
        <div class="equation-block">
            $$B(s_1^*, F(\cdot|\theta)) = B(s_1^*, F(\cdot|\theta')) \quad \text{for all } \theta, \theta'$$
        </div>
        <p>For the deterrence game with SR threshold \(\mu^*\), belief-robustness holds iff:</p>
        <div class="equation-block">
            $$\mu^* \notin [\beta,\; 1-\alpha] \quad \text{(the "danger zone")}$$
        </div>
        <p>When \(\mu^*\) lies <strong>below</strong> \(\beta\), SR always cooperates regardless of state. When \(\mu^*\) lies <strong>above</strong> \(1-\alpha\), SR always defects. In either case, the filtering belief gap is irrelevant.</p>
    </div>

    <h3>The Two Corrected Theorems</h3>
    
    <div class="theorem-box">
        <div class="box-title">Theorem 1' (Belief-Robust Markov Extension)</div>
        <p>Under belief-robustness, plus confound-defeating and NBC on the lifted space \(\tilde\Theta\):</p>
        <div class="equation-block">
            $$\boxed{\liminf_{\delta \to 1} \underline{U}_1(\delta) \geq V(s_1^*)}$$
        </div>
        <p>The bound is <strong>identical</strong> to the i.i.d. case. All proof machinery transfers without modification.</p>
    </div>

    <div class="theorem-box" style="border-left-color: #e67e22;">
        <div class="box-title" style="color: #e67e22;">Theorem 1'' (General Markov Extension)</div>
        <p>Without belief-robustness, for supermodular games with Markov states:</p>
        <div class="equation-block">
            $$\boxed{\liminf_{\delta \to 1} \underline{U}_1(\delta) \geq V_{\mathrm{Markov}}(s_1^*)}$$
        </div>
        <p>where</p>
        <div class="equation-block">
            $$V_{\mathrm{Markov}}(s_1^*) = \sum_{\theta \in \Theta} \pi(\theta) \cdot \inf_{(\alpha_0, \alpha_2) \in B(s_1^*, F(\cdot|\theta))} u_1(\theta, s_1^*(\theta), \alpha_2) \;\leq\; V(s_1^*)$$
        </div>
        <p>Equality holds <strong>if and only if</strong> the game is belief-robust. The gap \(V(s_1^*) - V_{\mathrm{Markov}}\) is the <strong>cost of persistence</strong>.</p>
    </div>

    <h3>Proof Structure: Where i.i.d. Is Actually Used</h3>
    
    <table>
        <thead>
            <tr>
                <th>Proof Step</th>
                <th>i.i.d. used?</th>
                <th>Modification for Markov</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Step 0: OT / Confound-defeating</td>
                <td>No</td>
                <td>Replace \(Y_0\) with \(\tilde\Theta\)</td>
            </tr>
            <tr style="background: rgba(231,76,60,0.08);">
                <td><strong>Step 1: Equilibrium (Lemma 1)</strong></td>
                <td><strong>Yes</strong></td>
                <td><strong>SR belief becomes state-contingent</strong></td>
            </tr>
            <tr>
                <td>Step 2: KL bound (Lemma 2)</td>
                <td>No</td>
                <td>None needed &mdash; identical bound!</td>
            </tr>
            <tr>
                <td>Step 3: Martingale (Lemma 3)</td>
                <td>Partially</td>
                <td>Ergodicity + filter stability</td>
            </tr>
            <tr>
                <td>Step 4: Combining (Lemma 4)</td>
                <td>No</td>
                <td>Uses corrected best response</td>
            </tr>
            <tr style="background: rgba(231,76,60,0.08);">
                <td><strong>Step 5: Payoff bound</strong></td>
                <td><strong>Yes</strong></td>
                <td><strong>Belief-robust: \(V(s_1^*)\); General: \(V_{\mathrm{Markov}}\)</strong></td>
            </tr>
        </tbody>
    </table>

    <p>The pattern: <strong>information-theoretic</strong> steps (KL bound, martingale) are process-independent, while <strong>game-theoretic</strong> steps (equilibrium implications, payoff bound) depend on the information structure and require belief-robustness or the corrected bound.</p>

    <h3>The KL Bound: Still No Mixing-Time Correction</h3>
    
    <div class="extension-box">
        <div class="box-title">Key Finding: Lemma 2 Extends Unchanged</div>
        <p>The KL-divergence counting bound requires <strong>no mixing-time correction</strong>:</p>
        <div class="equation-block">
            $$\bar{T}(\eta, \mu_0) = \frac{-2\log\mu_0(\omega_{s_1^*})}{\eta^2}$$
        </div>
        <p><strong>Identical to the i.i.d. case.</strong> This uses only the KL chain rule, Bayesian updating, and Pinsker's inequality &mdash; all valid for general stochastic processes. Verified via Monte Carlo simulation.</p>
    </div>

    <h3>Mixing Time vs. Limiting Payoff</h3>
    
    <div class="result-box">
        <div class="box-title">Role of Mixing Time</div>
        <p>The mixing time \(\tau_{\mathrm{mix}}\) affects only the <strong>rate of convergence</strong> (how large \(\delta\) must be), not the <strong>limiting payoff</strong>. As \(\delta \to 1\), the transient phase vanishes regardless of mixing time.</p>
    </div>

    <div class="controls">
        <h4>Mixing Time Visualization</h4>
        <div class="control-group">
            <label>View mixing for \(\alpha\) = <span id="alpha-mix-display">0.30</span>, \(\beta\) = <span id="beta-mix-display">0.50</span></label>
        </div>
    </div>

    <div class="plot-container">
        <div id="mixing-time-plot"></div>
    </div>

    <h3>Interpolation: From i.i.d. to Persistent</h3>

    <p>The framework continuously interpolates between three regimes:</p>

    <div class="comparison-grid">
        <div class="comparison-card">
            <h4>Fast Mixing (\(\alpha+\beta \to 1\))</h4>
            <p>\(F(\cdot|\theta) \to \pi\) for all \(\theta\)</p>
            <p>Belief-robustness holds generically</p>
            <p>\(V_{\mathrm{Markov}} = V(s_1^*)\)</p>
            <p><strong>Recovers Luo &amp; Wolitzky</strong></p>
        </div>
        <div class="comparison-card">
            <h4>Moderate Persistence</h4>
            <p>\(F(\cdot|\theta)\) deviates from \(\pi\)</p>
            <p>Belief-robustness may fail</p>
            <p>\(V_{\mathrm{Markov}} \leq V(s_1^*)\)</p>
            <p><strong>New result (this paper)</strong></p>
        </div>
    </div>
</div>
