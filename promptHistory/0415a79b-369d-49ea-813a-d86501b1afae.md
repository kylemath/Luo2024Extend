user:
<user_query>
@agent841_workspace/agent841subagent1PromptJob1.md 
</user_query>

assistant:
[Thinking] The user is referencing a file. Let me read it to understand what they want.
[Tool call] Read
  path: /Users/kylemathewson/mathTest/agent841_workspace/agent841subagent1PromptJob1.md

[Tool result] Read

assistant:
[Thinking] The user is pointing me to this file, which is a task assignment for extending Lemma 2 (the KL divergence counting bound) from i.i.d. states to Markov chains, in the context of a reputation paper by Luo & Wolitzky (2024). The task asks me to write a report to `agent841_workspace/agent841subagent1Report.md`.

Let me understand the task:
1. Extend Lemma 2 (KL divergence counting bound) to Markov chains
2. The key insight is that the bound holds without the i.i.d. assumption


3. I need to structure the report with a statement, proof sketch, key insight, verification, and caveats

The prompt is fairly self-contained with the mathematical reasoning already laid out, so I should focus on organizing this into a clear report rather than deriving everything from scratch.
This is a math task asking me to extend Lemma 2 (the KL divergence counting bound) from i.i.d. to Markovian states and write a report. Let me first gather context from the paper and existing reports, then produce the deliverable.
[Tool call] TodoWrite
  todos: [{"id":"1","content":"Read paper and first parse summary for context","status":"in_progress"},{"id":"2","content":"Derive the extended Lemma 2 for Markov chains","status":"pending"},{"id":"3","content":"Write report to agent841_workspace/agent841subagent1Report.md","status":"pending"}]
  merge: false

[Tool result] TodoWrite

assistant:
[Tool call] Glob
  glob_pattern: **/AgentReports/Feb16_840_firstParse.md
[Tool call] Glob
  glob_pattern: **/paper/Marginal_Reputation*equations*.tex

[Tool result] Glob

[Tool result] Glob

assistant:
[Tool call] Read
  path: /Users/kylemathewson/mathTest/AgentReports/Feb16_840_firstParse.md
[Tool call] Read
  path: /Users/kylemathewson/mathTest/paper/Marginal_Reputation Dec 17 2024_equations.tex

[Tool result] Read

[Tool result] Read

assistant:
[Thinking] Now I have all the context I need. Let me understand the task clearly:

1. The prompt asks me to extend Lemma 2 (KL divergence counting bound) from i.i.d. to Markov chains
2. The key insight (already noted in the prompt) is that the bound actually holds WITHOUT i.i.d. - it only needs Bayesian updating and Pinsker's inequality
3. I need to write a report with: Statement, Proof sketch, Key insight, Verification, Caveats

Let me look at the paper's Lemma 2 more carefully. From the first parse (Section 2.3) and the equations file, I can see:


Lemma 2 bounds the expected number of periods where the true state falls outside the confidence set, with the bound depending on the initial prior and a confidence parameter. The proof relies on a cumulative KL divergence argument—the total KL divergence across all periods is bounded by the negative log of the prior probability on the true state, and Pinsker's inequality then converts violations of the confidence set (measured in TV distance) into KL divergence contributions, yielding the final bound.

Now I'm working through the key step: establishing the total KL divergence bound using Bayesian updating and likelihood ratios, comparing the equilibrium measure against a deviation measure where the long-run player commits to the alternative action. where long-run player plays s₁*). The KL divergence of Q^T relative to P^T is:

D(Q^T || P^T) = Σ_{t=0}^{T-1} E_Q[D(Q_{y_t|h_t} || P_{y_t|h_t})]

This chain rule decomposition works because of the chain rule for KL divergence, which holds for ANY joint distribution - it does NOT require i.i.d.

Moreover, by the martingale property of likelihood ratios:

D(Q^T || P^T) = E_Q[log(dQ^T/dP^T)] 

And since dQ^T/dP^T is related to the posterior, we get:

E_Q[log(μ_T(ω_{s₁*})/μ₀(ω_{s₁*}))] ≤ 0 (since μ_T ≤ 1)

Actually, let me reconsider this more carefully. The key insight is that the cumulative KL divergence across all time steps bounds the log-odds ratio: the sum of conditional KL divergences between the two belief distributions at each history is bounded above by the negative log of the initial belief on the true strategy, which follows from the likelihood ratio martingale property and the fact that posterior beliefs are bounded by 1. The argument doesn't actually require the i.i.d. assumption at all. Now I'm applying Pinsker's inequality, which relates total variation distance to KL divergence pointwise—again, no independence needed. If the total variation distance between two distributions exceeds some threshold η, then their KL divergence must be at least 2η². So applying Pinsker's inequality in reverse: if the TV distance exceeds η, then the KL divergence must be greater than η²/2. This means the number of periods where the distributions diverge beyond η is bounded by the total KL budget divided by η²/2, which gives exactly the T̄(η, μ₀) bound from the paper. The crucial realization is that this entire argument holds for any stochastic process—the i.i.d. assumption never actually enters the proof, only Bayesian updating and the KL divergence constraint matter.

In the Markov setting, the key difference is that the signal distribution at each period depends on the current state θ_t. Under the commitment strategy s₁*, the signal distribution is determined by s₁*(θ_t), whereas under the equilibrium strategy σ₁*(h_t), it may deviate, creating the divergence between Q and P that the bound tracks.

The chain rule decomposition for KL divergence still applies here because it's fundamentally about conditional distributions along the sequence, independent of the underlying state process. Similarly, the total KL bound from Bayesian updating remains valid since it doesn't depend on whether the state follows a Markov chain or any other structure—it's a consequence of how posteriors evolve under observation.

So the counting bound T̄(η, μ₀) = -2log μ₀(ω_{s₁*})/η² holds exactly as stated, with no correction factor needed. The initial intuition about mixing time corrections was based on conflating the KL bound (which requires no i.i.d. assumption) with other concentration inequalities that do depend on independence.

Let me reconsider whether this argument is actually sound. The signal y_t depends on the actions chosen at time t, and while the state θ_t follows a Markov process, the key insight is that the signal distribution conditional on actions remains well-defined regardless of state history. Under the deviation to s₁*, the long-run behavior of the signal is what matters for the counting bound.

There's a technical point about Markov strategies—the commitment type would condition on both current and previous states—but the lifted-state framework handles this naturally. For Lemma 2, what really matters is comparing the signal distributions: under equilibrium play versus under the deviation, both conditional on the same history h_t.

Now I'm working through the KL divergence calculation between these signal distributions, which relates to the likelihood ratio of the observed signals under each scenario. The key insight is that the cumulative divergence across all periods is bounded by the log of the prior probability on the commitment type's strategy, which pins down how much the short-run player can learn from deviations. I'm working through the posterior update for the signal under Q versus P, where the mixture distribution under the prior acts as the normalizing constant. Actually, I should step back and apply the standard entropy bound result from Gossner more directly rather than deriving this from scratch. I'm working through the relationship between the posterior belief on ω_{s₁*} and the likelihood ratio, trying to establish a clean bound using the mixture distribution. Let me step back and use the standard KL divergence argument instead. I see the issue now—I need to distinguish between p_t as the equilibrium signal distribution versus the mixture distribution. The key is that P represents the equilibrium measure where the long-run player follows the equilibrium strategy σ₁*, while Q represents the deviation where they play s₁* instead, and the signal distribution under P depends on the equilibrium strategies of all players. Under Q, the long-run player commits to s₁* while the short-run players still follow their equilibrium strategies σ₀* and σ₂*. Now I'm working through the Bayesian updating: the equilibrium measure P is a mixture over types weighted by the prior, while the deviation measure Q corresponds to the specific type that would play s₁*. The likelihood ratio between these measures determines how beliefs about the type update based on the observed history. P^T(h_T)

So: dQ_T/dP_T = μ_T(ω_{s₁*}) / μ₀(ω_{s₁*})

Taking the log:
log(dQ_T/dP_T) = log μ_T(ω_{s₁*}) - log μ₀(ω_{s₁*})

Since μ_T(ω_{s₁*}) ≤ 1:
log(dQ_T/dP_T) ≤ -log μ₀(ω_{s₁*})

Now, using the chain rule:
log(dQ_T/dP_T) = Σ_{t=0}^{T-1} log(q_t(y_t|h_t)/p_t(y_t|h_t))

where q_t and p_t are the conditional signal distributions under Q and P respectively. Taking the expectation under Q, I can decompose this into a sum of KL divergences between the conditional distributions at each timestep. So E_Q[log(dQ_T/dP_T)] equals the sum of expected KL divergences across all time steps.

Now I'm checking whether I can directly bound this using the non-negativity of KL divergence, but I need to be more careful about how the pieces fit together. The key constraint is that log μ_T is at most zero since μ_T is a probability, and I can relate this back to the initial measure μ₀ through the likelihood ratio.

This gives me the crucial inequality: the sum of expected KL divergences across all timesteps is bounded by the negative log of the initial measure on the optimal state. What's important here is that this bound holds for any horizon T, and the derivation doesn't depend on any i.i.d. assumptions—it's purely a consequence of Bayesian updating and properties of log-likelihoods.

Now I need to translate this into a counting bound.

If a history h_t falls outside the "good" set H_t^η, then the total variation distance between the beliefs must exceed η. Using Pinsker's inequality to convert this to a KL divergence lower bound, I get that each such history contributes at least 2η² to the divergence. The paper's bound on the number of such histories should follow from dividing the total KL budget by this per-history cost.

Now I'm checking whether the constant in Pinsker's inequality matches what the paper uses—there are different conventions for how it's stated, and the factor of 4 discrepancy suggests I need to verify which version applies here. TV² ≤ D(P||Q)/2, which is the same thing.

With D ≥ 2||P-Q||², the counting bound would be T̄ ≤ -log μ₀/(2η²), but the paper gives T̄ = -2log μ₀/η², which is 4 times larger. Looking at equations 314-315, the paper uses D ≤ η²/2 to imply ||P-Q|| ≤ η, suggesting a weaker version of Pinsker's inequality. The discrepancy depends on how total variation distance is defined—whether as half the L1 distance or the full L1 distance—which changes whether D ≥ 2||P-Q||_TV² or D ≥ ||P-Q||²/2.

Now I'm thinking about the Markov case where the state θ_t evolves as a Markov chain, and at each period t, player 0 receives a signal y₀,t drawn from ρ(·|a₀) based on their action a₀ = α₀.

The key insight is that in the applications like deterrence and trust, y₀ equals θ and is drawn i.i.d., but the Markov extension allows θ_t to follow a Markov process instead. What matters for the KL divergence bound is actually the public signals y₁ and y₂, not the private signal y₀.

Now I'm working through how the public signal distribution changes between the commitment type and equilibrium play. Under commitment, the long-run player follows a fixed strategy based on the current state (or the lifted state if we're tracking history), while under equilibrium play they respond to both history and their type. The public signals depend on the long-run player's action, which differs between these two scenarios, and I need to track how Bayesian updating evolves across the state transitions. The Markov case doesn't require any adjustments to Lemma 2—the bound holds exactly as is, since the Bayesian updating mechanism depends only on posterior tracking, not on whether the underlying state process is i.i.d. or Markov. I was conflating the KL divergence argument, which needs no independence assumption, with other statistical tools that do require mixing conditions. Now I'll write up the report with the necessary rigor but keeping it concise.
Now I have all the context needed. Let me derive the result and write the report.
[Tool call] TodoWrite
  todos: [{"id":"1","content":"Read paper and first parse summary for context","status":"completed"},{"id":"2","content":"Derive the extended Lemma 2 for Markov chains","status":"in_progress"},{"id":"3","content":"Write report to agent841_workspace/agent841subagent1Report.md","status":"pending"}]
  merge: true

[Tool result] TodoWrite

assistant:
[Tool call] TodoWrite
  todos: [{"id":"2","content":"Derive the extended Lemma 2 for Markov chains","status":"completed"},{"id":"3","content":"Write report to agent841_workspace/agent841subagent1Report.md","status":"in_progress"}]
  merge: true
[Tool call] Write
  path: /Users/kylemathewson/mathTest/agent841_workspace/agent841subagent1Report.md
  contents: # Subagent 1 Report: KL Divergence Counting Bound for Markov Chains

**Task:** Extend Lemma 2 of Luo & Wolitzky (2024) from i.i.d. states to Markovian states.  
**Result:** The bound holds **exactly as stated** — no correction factor is needed.

---

## 1. Statement of Extended Lemma 2

**Lemma 2 (Markov Extension).** Fix a Nash equilibrium $(\sigma_0^*, \sigma_1^*, \sigma_2^*)$ and let $Q$ denote the probability measure over infinite histories induced by the deviation where the long-run player plays $s_1^*$ in every period (while players 0 and 2 continue to play equilibrium strategies). Let $\theta_t$ follow a stationary ergodic Markov chain with transition kernel $F(\cdot | \theta_{t-1})$ and stationary distribution $\pi$. Then:

$$E_Q\left[\#\left\{t : h_t \notin H_t^\eta\right\}\right] < \bar{T}(\eta, \mu_0) := \frac{-2\log \mu_0(\omega_{s_1^*})}{\eta^2}$$

where $H_t^\eta = \{h_t : \|p(\sigma_0^*, s_1^*, \sigma_2^* | h_t) - p(\sigma_0^*, \sigma_1^*, \sigma_2^* | h_t)\| \leq \eta\}$.

**This is identical to the i.i.d. bound.** No mixing-time correction $C(\tau_{\text{mix}})$ is required.

---

## 2. Proof Sketch

The proof has two steps: (A) a total KL divergence bound from Bayesian updating, and (B) conversion to a counting bound via Pinsker's inequality. Neither step uses i.i.d.

### Step A: Total KL Divergence Bound

**Claim.** For any $T \geq 1$:

$$\sum_{t=0}^{T-1} E_Q\left[D\!\left(p(\sigma_0^*, s_1^*, \sigma_2^* | h_t) \;\|\; p(\sigma_0^*, \sigma_1^*, \sigma_2^* | h_t)\right)\right] \leq -\log \mu_0(\omega_{s_1^*})$$

**Proof.** Let $P$ be the equilibrium measure (mixture over all types weighted by $\mu_0$) and $Q = P_{\omega_{s_1^*}}$ be the measure conditional on the long-run player being the commitment type $\omega_{s_1^*}$.

**Step A.1 (Bayes' rule).** By Bayes' rule, the posterior on $\omega_{s_1^*}$ satisfies:

$$\mu_T(\omega_{s_1^*} | h_T) = \mu_0(\omega_{s_1^*}) \cdot \frac{dQ^T(h_T)}{dP^T(h_T)}$$

where $dQ^T / dP^T$ is the Radon-Nikodym derivative (likelihood ratio) of the first $T$ signals under $Q$ versus $P$. Since $\mu_T(\omega_{s_1^*}) \leq 1$:

$$\frac{dQ^T(h_T)}{dP^T(h_T)} \leq \frac{1}{\mu_0(\omega_{s_1^*})}$$

**Step A.2 (Chain rule for log-likelihoods).** The log-likelihood ratio decomposes along the signal sequence:

$$\log \frac{dQ^T}{dP^T}(h_T) = \sum_{t=0}^{T-1} \log \frac{q_t(y_t | h_t)}{p_t(y_t | h_t)}$$

where $q_t(\cdot | h_t) = p(\sigma_0^*, s_1^*, \sigma_2^* | h_t)$ and $p_t(\cdot | h_t) = p(\sigma_0^*, \sigma_1^*, \sigma_2^* | h_t)$ are the period-$t$ signal distributions under $Q$ and $P$, respectively. **This decomposition is the chain rule for conditional densities and holds for any stochastic process** — it does not require independence across periods.

**Step A.3 (Taking expectations).** Taking $E_Q$ of both sides:

$$E_Q\left[\log \frac{dQ^T}{dP^T}\right] = \sum_{t=0}^{T-1} E_Q\left[E_Q\left[\log \frac{q_t(y_t | h_t)}{p_t(y_t | h_t)} \;\bigg|\; h_t\right]\right] = \sum_{t=0}^{T-1} E_Q\left[D(q_t(\cdot|h_t) \| p_t(\cdot|h_t))\right]$$

From Step A.1, $\log(dQ^T / dP^T) \leq -\log \mu_0(\omega_{s_1^*})$ pointwise, and since this holds for every history:

$$\sum_{t=0}^{T-1} E_Q\left[D(q_t(\cdot|h_t) \| p_t(\cdot|h_t))\right] \leq -\log \mu_0(\omega_{s_1^*})$$

**Crucially:** Steps A.1–A.3 use only Bayes' rule and the chain rule for conditional distributions. The state process $\{\theta_t\}$ can be i.i.d., Markov, or any adapted stochastic process — the bound is unchanged. $\square$

### Step B: Counting Bound via Pinsker's Inequality

**Claim.** If $h_t \notin H_t^\eta$, then $D(q_t(\cdot|h_t) \| p_t(\cdot|h_t)) > \eta^2/2$.

**Proof.** By Pinsker's inequality (a pointwise inequality, no distributional assumptions):

$$\|q_t(\cdot|h_t) - p_t(\cdot|h_t)\|_{TV} \leq \sqrt{2\,D(q_t(\cdot|h_t) \| p_t(\cdot|h_t))}$$

Contrapositive: if $\|q_t - p_t\| > \eta$, then $D(q_t \| p_t) > \eta^2 / 2$. $\square$

**Combining Steps A and B.** Let $\bar{N}$ be the expected number of periods where $h_t \notin H_t^\eta$. At each such period, $D_t := D(q_t(\cdot|h_t) \| p_t(\cdot|h_t)) > \eta^2/2$. Since KL divergences are non-negative:

$$\bar{N} \cdot \frac{\eta^2}{2} \leq \sum_{t=0}^{T-1} E_Q[D_t] \leq -\log \mu_0(\omega_{s_1^*})$$

$$\implies \bar{N} \leq \frac{-2\log \mu_0(\omega_{s_1^*})}{\eta^2} = \bar{T}(\eta, \mu_0)$$

This completes the proof. $\square$

---

## 3. Key Insight: Why i.i.d. Is Not Needed

The original proof of Lemma 2 **never uses the i.i.d. assumption**. The two ingredients are:

| Ingredient | What it requires | i.i.d. needed? |
|---|---|---|
| Total KL bound $\sum D_t \leq -\log \mu_0$ | Bayes' rule + chain rule for KL | **No.** Bayes' rule is valid for any stochastic process. The chain rule $D(P_{X,Y} \| Q_{X,Y}) = D(P_X \| Q_X) + E_P[D(P_{Y|X} \| Q_{Y|X})]$ holds for arbitrary joint distributions. |
| Pinsker's inequality $D \geq \eta^2/2$ when $\|P-Q\| > \eta$ | Pointwise property of divergence | **No.** Pinsker's inequality is a property of individual distributions, not of sequences. |
| Counting argument: if each "bad" period contributes $\geq \eta^2/2$ to a sum bounded by $K$, there are $\leq 2K/\eta^2$ bad periods | Arithmetic | **No.** This is just the pigeonhole principle. |

**The confusion about needing a mixing-time correction** arises from conflating two different uses of independence:

1. **KL divergence summation** (used here): The chain rule $D(Q^T \| P^T) = \sum E_Q[D(q_t \| p_t)]$ holds for **any** sequential process. This is because it is simply the telescoping of conditional KL divergences. Independence is irrelevant.

2. **Concentration inequalities** (NOT used here): Bounds like Hoeffding's inequality or the law of large numbers for $\frac{1}{T}\sum X_t$ do require independence or mixing. But Lemma 2 does not use concentration — it uses a **total sum bound**, not a rate-of-convergence bound.

The i.i.d. assumption in the paper is used elsewhere (Lemma 3's martingale convergence, the stationarity of the stage game), but Lemma 2 stands alone without it.

---

## 4. Verification: Recovery of the i.i.d. Case

When $F(\cdot | \theta_{t-1}) = \pi(\cdot)$ for all $\theta_{t-1}$ (i.i.d. states):

- The Markov chain has mixing time $\tau_{\text{mix}} = 0$ (already mixed).
- The lifted state $\tilde{\theta}_t = (\theta_t, \theta_{t-1})$ has distribution $\pi \otimes \pi$ (product, since states are independent).
- The bound reduces to $\bar{T}(\eta, \mu_0) = -2\log\mu_0(\omega_{s_1^*})/\eta^2$, which is **exactly** the paper's Lemma 2.

Since the bound is identical in the Markov and i.i.d. cases, recovery is trivially verified.

---

## 5. Caveats and Additional Remarks

### 5.1 No additional conditions needed for Lemma 2

The counting bound requires **no** ergodicity, stationarity, or finite mixing time. It holds for **any** state process — even non-stationary, non-ergodic, or adversarially chosen states. The only requirement is that the commitment type $\omega_{s_1^*}$ exists in the type space $\Omega$ with prior probability $\mu_0(\omega_{s_1^*}) > 0$.

### 5.2 What the commitment type plays in the Markov case

In the i.i.d. case, the commitment type plays $s_1^*: Y_0 \to \Delta(A_1)$ every period. In the Markov case with the lifted-state approach, the commitment type should play a **Markov strategy** $s_1^*: \Theta \times \Theta \to \Delta(A_1)$ (conditioning on $(\theta_t, \theta_{t-1})$). However, this does not affect Lemma 2 at all — the bound depends only on $\mu_0(\omega_{s_1^*})$, not on the form of $s_1^*$.

For the simpler case where the commitment type conditions only on $\theta_t$ (i.e., $s_1^*: \Theta \to \Delta(A_1)$, ignoring $\theta_{t-1}$), the bound still holds with no change.

### 5.3 The mixing time enters elsewhere, not here

The mixing time $\tau_{\text{mix}}$ is relevant for:
- **Lemma 3** (martingale convergence / belief merging): The rate at which the posterior $\mu_t$ concentrates on $\{\omega_R, \omega_{s_1^*}\}$ may depend on mixing properties when signals are dependent.
- **The OT / confound-defeating analysis**: The stationary distribution $\tilde{\rho}$ on the lifted state space determines the optimal transport problem.
- **The uniformity argument**: Compactness of the strategy space now involves Markov strategies.

But for Lemma 2 specifically, **the mixing time is irrelevant**.

### 5.4 Intuition for why the bound is process-independent

The KL divergence bound $\sum D_t \leq -\log \mu_0$ is fundamentally a statement about **Bayesian learning capacity**: there are only $-\log \mu_0$ "nats" of surprise available for distinguishing the commitment type from the equilibrium play. Each distinguishing period "uses up" at least $\eta^2/2$ nats. This budget constraint is purely information-theoretic and has nothing to do with the temporal structure of the signal-generating process.

---

## 6. Summary

| | i.i.d. case (paper) | Markov case (extension) |
|---|---|---|
| **Bound** | $\bar{T} = -2\log\mu_0(\omega_{s_1^*})/\eta^2$ | $\bar{T} = -2\log\mu_0(\omega_{s_1^*})/\eta^2$ |
| **Proof tools** | Bayes' rule + KL chain rule + Pinsker | Bayes' rule + KL chain rule + Pinsker |
| **Correction factor** | None | **None** |
| **Additional conditions** | $\mu_0(\omega_{s_1^*}) > 0$ | $\mu_0(\omega_{s_1^*}) > 0$ |

**Bottom line:** Lemma 2 extends to the Markov case **verbatim**. The i.i.d. assumption is not used in its proof. The first-parse conjecture that a mixing-time correction $C(\tau_{\text{mix}})$ would be needed was incorrect — the bound is tighter than initially expected.


[Tool result] TodoWrite

[Tool result] Write

assistant:
[Tool call] TodoWrite
  todos: [{"id":"3","content":"Write report to agent841_workspace/agent841subagent1Report.md","status":"completed"}]
  merge: true

[Tool result] TodoWrite

assistant:
The report has been written to `agent841_workspace/agent841subagent1Report.md`.

**Key finding:** The KL divergence counting bound (Lemma 2) extends to the Markov case **with no modification at all** — the bound is identical:

$$\bar{T}(\eta, \mu_0) = \frac{-2\log\mu_0(\omega_{s_1^*})}{\eta^2}$$

The initial conjecture from the first parse that a mixing-time correction factor $C(\tau_{\text{mix}})$ would be needed was **incorrect**. The proof of Lemma 2 uses only three ingredients, none of which require i.i.d.:

1. **Bayes' rule** for posterior updating (valid for any stochastic process)
2. **Chain rule for KL divergence** to decompose the total divergence into per-period terms (valid for any sequential process — this is just telescoping of conditional KL divergences)
3. **Pinsker's inequality** to convert TV distance bounds to KL bounds (a pointwise property, no distributional assumptions)

The total KL budget $-\log \mu_0(\omega_{s_1^*})$ comes purely from the Bayesian updating identity $\mu_T \leq 1$, which holds regardless of the state process. The counting bound then follows by pigeonhole: each distinguishing period costs at least $\eta^2/2$ nats from this fixed budget.

The mixing time $\tau_{\text{mix}}$ will be relevant for other parts of the proof (Lemma 3's martingale convergence, the OT analysis), but for Lemma 2 specifically, it plays no role.