% ================================================================
\section{Belief-Robustness: The Key New Concept}\label{sec:belief_robust}

The central obstacle to extending Theorem~1 from i.i.d.\ to Markov states is the behavior of short-run player beliefs. When the Stackelberg strategy reveals the state, short-run players learn $\theta_t$ and form beliefs about $\theta_{t+1}$ using the filtering distribution $F(\cdot|\theta_t)$, which generically differs from the stationary distribution $\pi$. This section formalizes the issue and introduces the condition under which it can be resolved.

\subsection{Filtering Beliefs}

\begin{definition}[Filtering Belief]\label{def:filtering}
Given a state-revealing Stackelberg strategy $s_1^*$ (i.e., $s_1^*(\theta) \neq s_1^*(\theta')$ for $\theta \neq \theta'$), the \textbf{filtering belief} in state $\theta$ is
\begin{equation}
    F(\cdot|\theta_t) = \Prob(\theta_{t+1} = \cdot \mid \theta_t),
\end{equation}
the one-step-ahead predictive distribution conditional on the current state.
\end{definition}

For the two-state chain $\Theta = \{G, B\}$ with parameters $(\alpha, \beta)$, the filtering beliefs are $F(G|G) = 1-\alpha$ and $F(G|B) = \beta$, while the stationary distribution gives $\pi(G) = \beta/(\alpha+\beta)$. The expected gap between the filtering belief and the stationary distribution can be computed in closed form:
\begin{equation}\label{eq:belief_gap}
    \E\bigl[|F(G|\theta_t) - \pi(G)|\bigr] = \frac{2\alpha\beta|1-\alpha-\beta|}{(\alpha+\beta)^2}.
\end{equation}
This quantity equals zero \textbf{if and only if} $\alpha + \beta = 1$, which is precisely the i.i.d.\ case. Both the numerator factor $|1-\alpha-\beta|$ and the product $\alpha\beta$ must be nonzero for the gap to be positive, confirming that any departure from the i.i.d.\ regime produces a permanent structural discrepancy. For the baseline parameters $(\alpha=\BaseAlpha, \beta=\BaseBeta)$, the expected gap is \BeliefGapBaseline.

\subsection{The Belief-Robustness Condition}

\begin{definition}[Belief-Robustness]\label{def:belief_robust}
A game $(u_1, u_2)$ with Stackelberg strategy $s_1^*$ and Markov chain $(\Theta, F)$ is \textbf{belief-robust} if the short-run player Nash correspondence satisfies
\begin{equation}\label{eq:belief_robust}
    B(s_1^*, F(\cdot|\theta)) = B(s_1^*, F(\cdot|\theta')) \quad \text{for all } \theta, \theta' \in \Theta.
\end{equation}
\end{definition}

The condition requires that the short-run player's best-response set is invariant to the revealed state. Under belief-robustness, the filtering belief gap documented in~\eqref{eq:belief_gap} becomes irrelevant for equilibrium behavior: SR plays the same action regardless of whether their belief about the next state is $F(\cdot|G)$ or $F(\cdot|B)$.

\subsection{When Does Belief-Robustness Hold?}

For the deterrence game with SR threshold $\mu^*$ (the belief level at which SR is indifferent between cooperating and defecting), belief-robustness admits a clean characterization.

\begin{proposition}\label{prop:br_condition}
Belief-robustness holds if and only if
\begin{equation}\label{eq:br_interval}
    \mu^* \notin \bigl[\min_\theta F(G|\theta),\; \max_\theta F(G|\theta)\bigr] = [\beta,\; 1-\alpha].
\end{equation}
\end{proposition}

\begin{proof}
The SR best response depends on whether $F(G|\theta_t) \gtrless \mu^*$. If $\mu^* < \beta$, then $F(G|\theta_t) \geq \beta > \mu^*$ for all $\theta_t$, so SR always cooperates. If $\mu^* > 1-\alpha$, then $F(G|\theta_t) \leq 1-\alpha < \mu^*$ for all $\theta_t$, so SR always defects. In either case, $B(s_1^*, F(\cdot|\theta))$ is constant across states. Conversely, if $\mu^* \in [\beta, 1-\alpha]$, there exist states $\theta, \theta'$ with $F(G|\theta) > \mu^* > F(G|\theta')$, so SR cooperates after $\theta$ and defects after $\theta'$, and belief-robustness fails.
\end{proof}

The economic interpretation is that belief-robustness fails precisely when the SR indifference threshold lies in the ``danger zone'' $[\beta, 1-\alpha]$---the interval spanned by the conditional beliefs across states. Three factors conspire to produce this failure: the game must have belief-sensitive SR behavior, with the threshold near $\pi$; the chain must be persistent enough that $F(\cdot|\theta)$ varies substantially across states; and the Stackelberg strategy must reveal state information to SR. When all three conditions hold simultaneously, persistence harms the long-run player's reputation value.

\begin{remark}[Baseline Example]
For the baseline parameters $(\alpha=\BaseAlpha, \beta=\BaseBeta)$, the danger zone is $[\BaseBeta, 1-\BaseAlpha] = [0.5, 0.7]$. The SR threshold $\mu^* = \SRThreshold$ lies inside this interval, so the baseline deterrence example is \textbf{not} belief-robust. However, changing SR payoffs to produce $\mu^* = \BRThreshold < \beta = \BaseBeta$ would place the threshold below the danger zone, restoring belief-robustness.
\end{remark}
