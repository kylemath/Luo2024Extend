% ================================================================
\section{The Extended Model}\label{sec:model}

We maintain all notation and conventions from Luo \& Wolitzky (2024, Sections~3.1--3.2), modifying only the state process.

\subsection{State Process}

Let $\Theta$ be a finite set.

\begin{assumption}[Markov States]\label{ass:markov}
The state $\theta_t \in \Theta$ follows a \textbf{stationary ergodic Markov chain} with:
\begin{enumerate}[label=(\alph*)]
    \item Transition kernel $F(\cdot | \theta)$ for each $\theta \in \Theta$, so that $\Prob(\theta_{t+1} = \theta' | \theta_t = \theta) = F(\theta' | \theta)$.
    \item Unique stationary distribution $\pi \in \Delta(\Theta)$ satisfying
    \begin{equation}\label{eq:stationary}
        \pi(\theta) = \sum_{\theta' \in \Theta} \pi(\theta') F(\theta | \theta') \quad \text{for all } \theta \in \Theta.
    \end{equation}
    \item The chain is \textbf{irreducible and aperiodic} (ensuring ergodicity).
\end{enumerate}
\end{assumption}

\begin{remark}
When $F(\cdot | \theta) = \pi(\cdot)$ for all $\theta$, the chain has no memory and we recover the i.i.d.\ case of Luo \& Wolitzky (2024). The two-state case with $\Theta = \{G, B\}$ is parameterized by $\alpha = \Prob(B|G)$ and $\beta = \Prob(G|B)$, giving $\pi(G) = \beta/(\alpha+\beta)$.
\end{remark}

\subsection{Lifted State Space}\label{subsec:lifted}

The central construction is the \emph{lifted state}:

\begin{definition}[Lifted State]\label{def:lifted}
Define
\begin{equation}\label{eq:lifted}
    \tilde\theta_t \;=\; (\theta_t,\, \theta_{t-1}) \;\in\; \tilde\Theta \;=\; \Theta \times \Theta.
\end{equation}
\end{definition}

\begin{remark}[Initial Period Convention]\label{rem:initial}
The lifted state $\tilde\theta_t = (\theta_t, \theta_{t-1})$ is defined for $t \geq 1$. For $t = 0$, we draw $\theta_{-1}$ from the stationary distribution $\pi$ independently of $\theta_0$ (equivalently, we initialize the chain in stationarity at $t = -1$). This loses nothing: the first period is transient and vanishes under $\delta \to 1$. Alternatively, one may start the game at $t = 1$.
\end{remark}

The process $(\tilde\theta_t)_{t \geq 1}$ is itself a Markov chain on $\tilde\Theta$ with transition probabilities
\begin{equation}
    \tilde F\bigl((\theta', \theta) \,\big|\, (\theta, \theta'')\bigr) \;=\; F(\theta' | \theta)
\end{equation}
and stationary distribution
\begin{equation}\label{eq:lifted_stationary}
    \tilde\rho(\theta, \theta') \;=\; \pi(\theta') \cdot F(\theta | \theta').
\end{equation}

\begin{proposition}\label{prop:lifted_ergodic}
Under Assumption~\ref{ass:markov}, the lifted chain $(\tilde\theta_t)$ on $\tilde\Theta$ is ergodic with unique stationary distribution $\tilde\rho$.
\end{proposition}

\begin{proof}
Since the original chain is irreducible on $\Theta$, for any states $\theta, \theta' \in \Theta$, there exists $n \in \N$ such that $F^n(\theta' | \theta) > 0$. Now consider two lifted states $(\theta_a, \theta_b)$ and $(\theta_d, \theta_c)$ in $\tilde\Theta$. By irreducibility of the original chain, there exists a finite path $\theta_a \to \theta_{i_1} \to \cdots \to \theta_{i_k} \to \theta_c$ with positive probability. This path in the original chain induces a path $(\theta_a, \theta_b) \to (\theta_{i_1}, \theta_a) \to \cdots \to (\theta_c, \theta_{i_k}) \to (\theta_d, \theta_c)$ in the lifted chain (where the final step uses $F(\theta_d | \theta_c) > 0$ for some path from $\theta_c$ to $\theta_d$). Hence the lifted chain is irreducible. Aperiodicity follows from aperiodicity of the original chain: if $F(\theta | \theta) > 0$ for some $\theta$, then $(\theta, \theta) \to (\theta, \theta)$ is a self-loop in the lifted chain. Uniqueness of $\tilde\rho$ follows from the Perron--Frobenius theorem.
\end{proof}

\begin{remark}[Effective State Space]\label{rem:effective}
If $F(\theta | \theta') = 0$ for some pair, the lifted state $(\theta, \theta')$ is never visited. The effective state space is $\tilde\Theta_+ = \{(\theta, \theta') \in \Theta \times \Theta : F(\theta | \theta') > 0\} \subseteq \tilde\Theta$. All results hold on $\tilde\Theta_+$; we write $\tilde\Theta$ for notational simplicity throughout.
\end{remark}

\begin{remark}[Purpose of the Lifting]\label{rem:key_property}
The lifted state provides a Markov structure on which the optimal transport framework and cyclical monotonicity characterizations apply. The \textbf{key property} is that $\tilde\theta_t$ has a \emph{fixed, known} stationary distribution $\tilde\rho$, playing precisely the role of the i.i.d.\ signal distribution $\rho$ in Luo \& Wolitzky (2024). This is the central insight enabling the extension.
\end{remark}

\subsection{Stage Game}

The stage game is identical to Luo \& Wolitzky's Section~3.1, except:
\begin{enumerate}[label=(\roman*)]
    \item The long-run player's private information each period is $\tilde\theta_t = (\theta_t, \theta_{t-1})$.
    \item A stage-game strategy for player~1 is $s_1 : \tilde\Theta \to \Delta(A_1)$, a \emph{Markov strategy}.
    \item Payoffs depend on the current state: $u_1(\theta_t, a_1, \alpha_2)$.
\end{enumerate}

We restrict throughout to payoffs $u_1(\theta_t, a_1, \alpha_2)$ that depend on $\theta_t$ alone (not the full lifted state $\tilde\theta_t$). This covers all standard applications---deterrence, trust, signaling---and avoids unmotivated generalization.

\subsection{Joint Distribution and Marginals}

Under Markov strategy $s_1$ and the stationary distribution $\tilde\rho$, the joint distribution over $(\tilde\theta, a_1)$ is:
\begin{equation}\label{eq:joint}
    \gamma(s_1)[\tilde\theta, a_1] \;=\; \tilde\rho(\tilde\theta) \cdot s_1(\tilde\theta)[a_1].
\end{equation}
The marginals of $\gamma(s_1)$ are the stationary distribution $\pi_{\tilde\Theta}(\gamma) = \tilde\rho$, which is fixed and known, and the action marginal $\pi_{A_1}(\gamma) = \phi(s_1) = \sum_{\tilde\theta} \tilde\rho(\tilde\theta)\, s_1(\tilde\theta)[\cdot]$, which is observable to the short-run players. These two marginals play exactly the roles of the exogenous state distribution and the action frequency in the Luo--Wolitzky optimal transport formulation.

\subsection{Commitment Types}

A commitment type $\omega_{s_1} \in \Omega$ plays Markov strategy $s_1 : \tilde\Theta \to \Delta(A_1)$ every period. The type space $\Omega$ is countable with full-support prior $\mu_0 \in \Delta(\Omega)$.

\begin{remark}
A ``memoryless'' commitment type that plays $s_1 : \Theta \to \Delta(A_1)$ (ignoring $\theta_{t-1}$) is a special case. The framework allows richer types that condition on transitions, but the memoryless case suffices for most applications.
\end{remark}

\subsection{Repeated Game}

The repeated game structure is identical to Luo \& Wolitzky's Section~3.2. Assumption~1 (signal $y_1$ identifies $a_1$) is maintained throughout.

\begin{remark}[Within-Period Timing]\label{rem:timing}
Following Luo \& Wolitzky (2024, Section~3.1), players~1 and~2 move \textbf{simultaneously} within each period. Signals $y_1$ and $y_2$ are generated after actions are chosen and observed publicly, becoming part of the history $h_t = (y_{1,t'}, y_{2,t'})_{t'=0}^{t-1}$ available in subsequent periods. In particular, the short-run player's information set at time~$t$ is $h_{t-1}$: SR does \emph{not} observe the long-run player's current action $a_{1,t}$ (or the current state $\theta_t$) before choosing $a_{2,t}$. Under a state-revealing commitment strategy $s_1^*$, the history $h_{t-1}$ reveals $\theta_0, \ldots, \theta_{t-1}$, so SR's belief about the current payoff-relevant state $\theta_t$ is the one-step-ahead predictive distribution $F(\cdot|\theta_{t-1})$.
\end{remark}
