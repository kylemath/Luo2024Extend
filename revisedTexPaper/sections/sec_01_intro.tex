% ================================================================
\section{Introduction}\label{sec:intro}

Luo \& Wolitzky (2024) establish a striking connection between reputation theory in repeated games and optimal transport theory. Their main result, Theorem~1, shows that a patient long-run player can secure her \emph{commitment payoff} $V(s_1^*)$ in any Nash equilibrium, provided her Stackelberg strategy $s_1^*$ is \emph{confound-defeating} and \emph{not behaviorally confounded}. Throughout their analysis, states are drawn \textbf{i.i.d.\ across periods}. The authors note (footnote~9) that the extension to persistent states is an open question.

\subsection{The Challenge of Markov States}

The extension from i.i.d.\ to Markov states introduces a fundamental new phenomenon. We employ a \emph{lifted state} construction $\tilde\theta_t = (\theta_t, \theta_{t-1})$. Since the original chain already possesses a stationary distribution $\pi$, the purpose of lifting is not to create stationarity but to encode Markov private information into a type space $\tilde\Theta = \Theta \times \Theta$ with joint stationary distribution $\tilde\rho$, playing the role of the exogenous signal distribution in Luo--Wolitzky's optimal transport formulation. However, the extension is not a straightforward substitution: under the simultaneous-move timing of Luo--Wolitzky (2024, Section~3.1), the short-run player does not observe the long-run player's current action before choosing. When the Stackelberg strategy is state-revealing (e.g., $s_1^*(G)=A$, $s_1^*(B)=F$), the public history reveals past states, so the short-run player's belief about the current payoff-relevant state $\theta_t$ is the filtering distribution $F(\cdot|\theta_{t-1})$ rather than the stationary distribution $\pi$. This creates a permanent structural gap: short-run behavior becomes state-contingent, and the Nash correspondence $B(s_1^*)$ must be replaced by a state-dependent object $B(s_1^*, F(\cdot|\theta_{t-1}))$.

\subsection{Main Results}

These observations lead to two main theorems. Theorem~1$'$ addresses the \emph{belief-robust} case: when the short-run player's best-response set $B(s_1^*, F(\cdot|\theta))$ is constant across states $\theta$---a condition we call \emph{belief-robustness}---the i.i.d.\ bound $V(s_1^*)$ holds exactly. The entire proof machinery (KL bound, OT robustness, monotonicity) applies without modification; belief-robustness ensures the filtering belief gap is irrelevant.

Theorem~1$''$ handles the general case. For all supermodular games with Markov states, a corrected bound holds:
\[
    V_{\mathrm{Markov}}(s_1^*) := \sum_{\theta' \in \Theta} \pi(\theta') \cdot \inf_{(\alpha_0, \alpha_2) \in B(s_1^*, F(\cdot|\theta'))} \sum_{\theta \in \Theta} F(\theta|\theta') \cdot u_1\bigl(\theta,\, s_1^*(\theta, \theta'),\, \alpha_2\bigr),
\]
where $\theta'$ indexes the previous state (determining SR's belief) and $\theta$ indexes the current state (entering the payoff). The bound satisfies $V_{\mathrm{Markov}} = V(s_1^*)$ if and only if the game is belief-robust. The difference $V(s_1^*) - V_{\mathrm{Markov}}$---the \emph{effect of persistence}---can be positive or negative, quantifying how state persistence affects reputation value. This is a new economic object that the i.i.d.\ framework cannot capture.

\subsection{Outline}

Section~\ref{sec:model} presents the model with the lifted state construction. Section~\ref{sec:belief_robust} introduces the key new concept of belief-robustness. Section~\ref{sec:theorems} states the two main theorems. Section~\ref{sec:proof} contains the proof sketch, tracing each step of the Luo--Wolitzky argument and identifying where modifications are needed for Markov states. Section~\ref{sec:supermodular} extends the supermodular case. Section~\ref{sec:example} works out the deterrence game in both belief-robust and non-belief-robust versions. Section~\ref{sec:interpolation} discusses the continuous interpolation between i.i.d.\ and persistent states and the economic implications. Section~\ref{sec:discussion} discusses open questions. Appendix~\ref{app:kl} verifies the KL chain rule and filter stability. Appendix~\ref{app:computational} documents the computational framework.
