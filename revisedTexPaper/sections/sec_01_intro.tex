% ================================================================
\section{Introduction}\label{sec:intro}

Luo \& Wolitzky (2024) establish a striking connection between reputation theory in repeated games and optimal transport theory. Their main result, Theorem~1, shows that a patient long-run player can secure her \emph{commitment payoff} $V(s_1^*)$ in any Nash equilibrium, provided her Stackelberg strategy $s_1^*$ is \emph{confound-defeating} and \emph{not behaviorally confounded}. Throughout their analysis, states are drawn \textbf{i.i.d.\ across periods}. The authors note (footnote~9) that the extension to persistent states is an open question.

\subsection{The Challenge of Markov States}

The extension from i.i.d.\ to Markov states introduces a fundamental new phenomenon. We employ a \emph{lifted state} construction $\tilde\theta_t = (\theta_t, \theta_{t-1})$, which provides a stationary distribution $\tilde\rho$ on the expanded space and allows the optimal transport framework to apply directly. However, the extension is not a straightforward substitution: when the Stackelberg strategy reveals the state (e.g., $s_1^*(G)=A$, $s_1^*(B)=F$), the short-run player learns $\theta_t$ exactly, and their belief about $\theta_{t+1}$ becomes the \emph{filtering distribution} $F(\cdot|\theta_t)$ rather than the stationary distribution $\pi$. This creates a permanent structural gap: short-run behavior becomes state-contingent, and the Nash correspondence $B(s_1^*)$ must be replaced by a state-dependent object $B(s_1^*, F(\cdot|\theta))$. Recognizing this---through a combination of expert feedback (Luo, 2026) and systematic computational verification---is the key insight of this paper.

\subsection{Computational Verification}

We conducted systematic computational analysis across seven diagnostic modules producing eight figures to characterize precisely which elements of the i.i.d.\ proof extend to the Markov setting and which require modification.

On the positive side, the KL counting bound extends verbatim to Markov processes, requiring no mixing-time correction; this was verified via $N=\KLMonteCarloN$ Monte Carlo simulations over $T=\KLMonteCarloPeriods$ periods. Filter stability holds with exponential forgetting, with fitted decay rate correlating at $r > \FilterCorrelation$ with the chain's second eigenvalue $|1-\alpha-\beta|$. The optimal transport support is robust to the belief perturbations that arise from Markov dynamics, with stability margin at least $\OTStabilityMargin$ in \OTStabilityPct\% of the $(\alpha,\beta)$ parameter space. Finally, the monotonicity characterization extends to the lifted state space for payoffs depending only on the current state~$\theta_t$.

On the negative side, short-run player beliefs permanently deviate from the stationary distribution, with mean total variation distance \TVMean{} and an analytical gap of~\BeliefGapBaseline{} for the baseline parameters. The Nash correspondence $B(s_1^*, \mu)$ varies period-to-period, producing a \SRDisagreement\% disagreement rate in short-run player actions between the stationary and filtered scenarios. The commitment payoff is consequently overestimated: \PayoffStationary{} under the stationary assumption versus \PayoffFiltered{} under filtered beliefs, a gap of \PayoffOverestimation\%.

\subsection{Corrected Results}

These findings guide two corrected theorems. Theorem~1$'$ addresses the \emph{belief-robust} case: when the short-run player's best-response set $B(s_1^*, F(\cdot|\theta))$ is constant across states $\theta$---a condition we call \emph{belief-robustness}---the i.i.d.\ bound $V(s_1^*)$ holds exactly. The entire proof machinery (KL bound, OT robustness, monotonicity) applies without modification; belief-robustness ensures the filtering belief gap is irrelevant.

Theorem~1$''$ handles the general case. For all supermodular games with Markov states, a corrected bound holds:
\[
    V_{\mathrm{Markov}}(s_1^*) := \sum_{\theta \in \Theta} \pi(\theta) \cdot \inf_{(\alpha_0, \alpha_2) \in B(s_1^*, F(\cdot|\theta))} u_1(\theta, s_1^*(\theta), \alpha_2) \;\leq\; V(s_1^*),
\]
with equality if and only if the game is belief-robust. The gap $V(s_1^*) - V_{\mathrm{Markov}}$ quantifies the ``cost of persistence'' in reputation games---a new economic object that the i.i.d.\ framework cannot capture.

\subsection{A Note on Process}

This paper documents not only the mathematical results but also the research process that produced them: AI-assisted conjecture, expert feedback from a co-author of the original paper (Luo, 2026), systematic computational verification via hierarchical AI agents, and iterative revision. Section~\ref{sec:methodology} provides a detailed account, including the agent architecture, the computational testing framework, and the timeline from initial challenge to final paper. We include this methodological documentation because the process---rapid AI-assisted exploration refined through expert dialogue and computational evidence---may itself be of interest as a model for future human--AI collaboration in mathematical research.

\subsection{Outline}

Section~\ref{sec:model} presents the model with the lifted state construction. Section~\ref{sec:belief_robust} introduces the key new concept of belief-robustness. Section~\ref{sec:theorems} states the two corrected theorems. Section~\ref{sec:proof} contains the proof sketch, tracing each step of the Luo--Wolitzky argument and identifying where corrections are needed. Section~\ref{sec:supermodular} extends the supermodular case. Section~\ref{sec:example} works out the deterrence game in both belief-robust and non-belief-robust versions. Section~\ref{sec:interpolation} discusses the continuous interpolation between i.i.d.\ and persistent states. Section~\ref{sec:methodology} gives a detailed account of the human--AI collaboration process across both phases. Section~\ref{sec:discussion} discusses open questions. Appendix~\ref{app:kl} verifies the KL chain rule and filter stability. Appendix~\ref{app:computational} documents the reproducible computational framework.
