% ================================================================
\section{Main Theorems}\label{sec:theorems}

We state two results. Theorem~1$'$ recovers the exact i.i.d.\ bound under belief-robustness. Theorem~1$''$ provides the general Markov bound.

\subsection{Definitions on the Expanded State Space}

All definitions from Luo \& Wolitzky (2024) carry over to $\tilde\Theta$, with strategies mapping $\tilde\Theta \to \Delta(A_1)$.

\begin{definition}[Confound-Defeating, Extended]\label{def:CD_extended}
A Markov strategy $s_1^* : \tilde\Theta \to \Delta(A_1)$ is \textbf{confound-defeating} if for every $(\alpha_0, \alpha_2) \in B_0(s_1^*)$, the joint distribution $\gamma(\alpha_0, s_1^*)$ is the \emph{unique solution} to:
\begin{equation}\label{eq:OT_extended}
    \OT\bigl(\tilde\rho(\alpha_0),\, \phi(\alpha_0, s_1^*);\, \alpha_2\bigr): \quad \max_{\gamma \in \Delta(\tilde\Theta \times A_1)} \int u_1(\tilde\theta, a_1, \alpha_2)\, d\gamma
\end{equation}
subject to $\pi_{\tilde\Theta}(\gamma) = \tilde\rho(\alpha_0)$ and $\pi_{A_1}(\gamma) = \phi(\alpha_0, s_1^*)$.
\end{definition}

\begin{definition}[Not Behaviorally Confounded, Extended]\label{def:NBC_extended}
$s_1^*$ is \textbf{not behaviorally confounded} if for any $\omega_{s_1'} \in \Omega$ with $s_1' \neq s_1^*$ and any $(\alpha_0, \alpha_2) \in B_1(s_1^*)$, we have $p(\alpha_0, s_1^*, \alpha_2) \neq p(\alpha_0, s_1', \alpha_2)$.
\end{definition}

\subsection{Theorem~1\texorpdfstring{$'$}{'} (Belief-Robust Extension)}

\begin{theorem}[Belief-Robust Markov Extension]\label{thm:belief_robust}
Let $\theta_t$ follow a stationary ergodic Markov chain on finite $\Theta$ (Assumption~\ref{ass:markov}). Let $\tilde\theta_t = (\theta_t, \theta_{t-1})$ with stationary distribution $\tilde\rho$. Suppose:
\begin{enumerate}[label=(\roman*)]
    \item $\omega_{s_1^*} \in \Omega$, where $s_1^* : \tilde\Theta \to \Delta(A_1)$ is a Markov strategy;
    \item $s_1^*$ is confound-defeating on $\tilde\Theta$ (Definition~\ref{def:CD_extended});
    \item $s_1^*$ is not behaviorally confounded (Definition~\ref{def:NBC_extended});
    \item The game is \textbf{belief-robust} with respect to $s_1^*$ and $(\Theta, F)$ (Definition~\ref{def:belief_robust}).
\end{enumerate}
Then:
\begin{equation}\label{eq:thm_br}
    \boxed{\liminf_{\delta \to 1}\, \underline{U}_1(\delta) \;\geq\; V(s_1^*)}
\end{equation}
where $V(s_1^*) = \inf_{(\alpha_0, \alpha_2) \in B(s_1^*)} u_1(\alpha_0, s_1^*, \alpha_2)$ is the commitment payoff, identical to the i.i.d.\ case.
\end{theorem}

\begin{remark}
Under belief-robustness, the SR belief gap is irrelevant: SR plays the same best response regardless of the filtering belief $F(\cdot|\theta)$. All the confirmed proof machinery---KL counting bound, OT robustness, monotonicity---applies without modification.
\end{remark}

\subsection{Theorem~1\texorpdfstring{$''$}{''} (General Corrected Bound)}

\begin{definition}[Markov Commitment Payoff]\label{def:V_markov}
The \textbf{Markov commitment payoff} is
\begin{equation}\label{eq:V_markov}
    V_{\mathrm{Markov}}(s_1^*) := \sum_{\theta' \in \Theta} \pi(\theta') \cdot \inf_{(\alpha_0, \alpha_2) \in B(s_1^*, F(\cdot|\theta'))} \sum_{\theta \in \Theta} F(\theta|\theta') \cdot u_1\bigl(\theta,\, s_1^*(\theta, \theta'),\, \alpha_2\bigr).
\end{equation}
Here $\theta'$ indexes the \emph{previous} state $\theta_{t-1}$, which is known to SR through the state-revealing strategy and determines the filtering belief $F(\cdot|\theta')$ about the current state $\theta_t$ (Remark~\ref{rem:timing}). The variable $\theta$ indexes the \emph{current} state $\theta_t$, which enters the payoff $u_1$. The formula averages over previous states using the stationary distribution $\pi$, applies the \textbf{state-contingent} Nash correspondence $B(s_1^*, F(\cdot|\theta'))$ determined by the previous state, and takes the conditional expectation of the payoff over the current state.
\end{definition}

\begin{theorem}[General Markov Extension]\label{thm:general}
Under conditions (i)--(iii) of Theorem~\ref{thm:belief_robust}, with ergodic Markov states and confound-defeating $s_1^*$ on $\tilde\Theta$:
\begin{equation}\label{eq:thm_general}
    \boxed{\liminf_{\delta \to 1}\, \underline{U}_1(\delta) \;\geq\; V_{\mathrm{Markov}}(s_1^*)}
\end{equation}
with $V_{\mathrm{Markov}}(s_1^*) = V(s_1^*)$ if and only if the game is belief-robust.
\end{theorem}

\begin{remark}[Necessity of Belief-Robustness]\label{rem:necessity}
The ``if and only if'' in Theorem~\ref{thm:general} requires comment. \emph{Sufficiency} is straightforward: belief-robustness forces the same Nash correspondence in every state, so the state-contingent infima coincide with the unconditional infimum. \emph{Necessity} holds under a genericity condition: when $u_1$ has strictly increasing differences in $(\theta, a_1)$ (as assumed in the supermodular case, Proposition~\ref{prop:supermodular}) and the SR threshold $\mu^*$ lies in the interior of the interval $[\min_\theta F(G|\theta), \max_\theta F(G|\theta)]$, the state-contingent best-response sets differ strictly across states, producing different infima. For non-generic parameters---where distinct best-response sets happen to yield the same infimum value---$V_{\mathrm{Markov}} = V(s_1^*)$ could hold without belief-robustness.
\end{remark}

\begin{remark}[Relationship Between Theorems]
The two results are nested: Theorem~\ref{thm:belief_robust} is the special case of Theorem~\ref{thm:general} where belief-robustness forces $V_{\mathrm{Markov}} = V(s_1^*)$. The difference $V(s_1^*) - V_{\mathrm{Markov}}$---the \emph{effect of persistence}---can be positive or negative. When the stationary belief $\pi$ induces favorable SR behavior (e.g., $\pi(G) > \mu^*$ so SR cooperates under i.i.d.), persistence can only cause some states to trigger defection, yielding $V_{\mathrm{Markov}} \leq V(s_1^*)$. Conversely, when $\pi$ induces unfavorable SR behavior (e.g., $\pi(G) < \mu^*$ so SR always defects under i.i.d.), persistence can enable state-contingent cooperation, yielding $V_{\mathrm{Markov}} > V(s_1^*)$---a new economic phenomenon absent from the i.i.d.\ framework.
\end{remark}

\begin{remark}[Continuity in Chain Parameters]
$V_{\mathrm{Markov}}(s_1^*)$ is a continuous function of the chain parameters $(\alpha, \beta)$. As $\alpha + \beta \to 1$ (the i.i.d.\ limit), $F(\cdot|\theta) \to \pi(\cdot)$ for all $\theta$, so $V_{\mathrm{Markov}} \to V(s_1^*)$. The difference $V(s_1^*) - V_{\mathrm{Markov}}$ vanishes continuously and may be positive or negative away from the i.i.d.\ line.
\end{remark}

\subsection{Extension to Behaviorally Confounded Strategies (Theorem 2)}\label{subsec:theorem2}

The salience-based extension (Luo \& Wolitzky, 2024, Appendix~A, Theorem~2) also generalizes to the Markov setting.

\begin{theorem}[Extended Theorem 2]\label{thm:theorem2}
Under the same Markov setup, if $s_1^*$ is confound-defeating on $\tilde\Theta$ and has salience $\beta_s$ (defined identically to Luo \& Wolitzky, 2024, but with confounding weights computed on $\tilde\Theta$), then:
\begin{equation}
    \liminf_{\delta \to 1}\, \underline{U}_1(\delta) \;\geq\; \beta_s\, V_{\mathrm{Markov}}(s_1^*) + (1 - \beta_s)\, V_0(s_1^*).
\end{equation}
Under belief-robustness, $V_{\mathrm{Markov}}$ is replaced by $V(s_1^*)$. If $s_1^*$ is not behaviorally confounded, $\beta_s = 1$ and this reduces to Theorems~\ref{thm:belief_robust} or~\ref{thm:general} respectively.
\end{theorem}

\begin{proof}[Proof sketch]
The proof of Theorem~2 in Luo \& Wolitzky (2024) follows from Theorem~1 via Lemma~7 (the salience bound). Lemma~7 uses the submartingale property of $\mu_t(\omega_{s_1^*} | \Omega_\eta(s_1^*) \setminus \{\omega^R\}, h_t)$, which holds by Bayesian updating regardless of the signal process. The remainder of the argument---compactness, limiting, the three-case analysis---extends as in Section~\ref{sec:proof}. The only modification is the payoff bound: under belief-robustness, the full $V(s_1^*)$ is used; in the general case, $V_{\mathrm{Markov}}(s_1^*)$ replaces $V(s_1^*)$.
\end{proof}
