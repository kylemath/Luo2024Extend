% ================================================================
\section{Interpolation Between i.i.d.\ and Persistent}\label{sec:interpolation}

Our framework provides a continuous interpolation between the i.i.d.\ setting (Luo--Wolitzky 2024) and increasingly persistent Markov states.

\subsection{The Interpolation Landscape}

The interpolation is two-dimensional in the $(\alpha, \beta)$ parameter space:

\begin{itemize}
    \item \textbf{Along $\alpha+\beta = 1$ (i.i.d.\ line):} $F(\cdot|\theta) = \pi(\cdot)$ for all $\theta$, so $V_{\mathrm{Markov}} = V(s_1^*)$. No gap.
    \item \textbf{Away from $\alpha+\beta = 1$:} $V_{\mathrm{Markov}} < V(s_1^*)$, with gap increasing as $|1-\alpha-\beta|$ grows.
    \item \textbf{Corners $(\alpha,\beta) \to (0,0)$ (near-perfect persistence):} $V_{\mathrm{Markov}} \to$ state-by-state payoff; gap maximized.
\end{itemize}

The mean TV distance $\|F(\cdot|\theta) - \pi\|$ averaged over the parameter space is \TVMean{} (Figure~\ref{fig:belief_deviation}), confirming that belief deviation from the stationary distribution is the norm, not the exception, for Markov states.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig_belief_deviation.png}
\caption{Mean TV distance $\|F(\cdot|\theta) - \pi\|$ across the $(\alpha,\beta)$ parameter space. The deviation vanishes along $\alpha+\beta=1$ (i.i.d.) and increases toward the corners (high persistence). Average: \TVMean.}
\label{fig:belief_deviation}
\end{figure}

\subsection{Recovery of Existing Results}

\begin{description}
    \item[i.i.d.\ (Luo--Wolitzky 2024):] $F(\cdot|\theta) = \pi(\cdot)$ for all $\theta$. Theorems~\ref{thm:belief_robust} and~\ref{thm:general} both reduce to the original Theorem~1 with $V_{\mathrm{Markov}} = V(s_1^*)$.

    \item[Perfectly persistent (Pei 2020):] $F(\cdot|\theta) = \delta_\theta$. The chain is not ergodic, so our framework does not directly apply. As $\alpha, \beta \to 0$, the gap $V(s_1^*) - V_{\mathrm{Markov}}$ diverges (the rate of convergence in $\delta$ degrades), and Pei's different approach is needed.
\end{description}

\subsection{The Cost of Persistence}

The gap $V(s_1^*) - V_{\mathrm{Markov}}$ is a new economic object: the \emph{cost of persistence in reputation games}. It quantifies how much the LR player loses because state persistence causes SR to adjust behavior state-by-state.

For the deterrence game with $\mu^* = \SRThreshold$:
\begin{itemize}
    \item The cost is $\PayoffStationary - \PayoffFiltered = \PayoffGapAbsolute$ (\PayoffOverestimation\% of the i.i.d.\ payoff).
    \item The cost is increasing in $|1-\alpha-\beta|$ (persistence).
    \item The cost vanishes as $\alpha+\beta \to 1$ (i.i.d.\ limit).
\end{itemize}

This provides a direct link between the dynamics of the economic environment and the value of reputation.
