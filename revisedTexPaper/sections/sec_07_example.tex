% ================================================================
\section{Worked Example: Deterrence Game}\label{sec:example}

We illustrate both theorems using the deterrence game with Markov attacks, presenting a belief-robust and a non-belief-robust version.

\subsection{Setup}

The state $\theta_t \in \{G, B\}$ follows a Markov chain with $\alpha = \Prob(B|G) = \BaseAlpha$ and $\beta = \Prob(G|B) = \BaseBeta$. The stationary distribution is $\pi(G) = \BasePiG$, $\pi(B) = \BasePiB$.

The LR player chooses $a_1 \in \{A(\text{cquiesce}), F(\text{ight})\}$; the SR player chooses $a_2 \in \{C(\text{ooperate}), D(\text{efect})\}$. The Stackelberg strategy is $s_1^*(G)=A$, $s_1^*(B)=F$.

\subsection{Version 1: Belief-Robust ($\mu^* = \BRThreshold$)}

With SR payoffs calibrated so the indifference threshold is $\mu^* = \BRThreshold < \beta = \BaseBeta$:

Since $\mu^* = \BRThreshold < \beta = \BaseBeta \leq F(G|\theta)$ for all $\theta$, the SR player always cooperates regardless of the revealed state. The game is \textbf{belief-robust} (Proposition~\ref{prop:br_condition}).

By Theorem~\ref{thm:belief_robust}:
\[
    \liminf_{\delta\to 1} \underline{U}_1(\delta) \geq V(s_1^*) = \BRPayoff.
\]
The bound is exact and identical to the i.i.d.\ case.

\subsection{Version 2: Non-Belief-Robust ($\mu^* = \SRThreshold$)}

With SR payoffs giving threshold $\mu^* = \SRThreshold \in [\BaseBeta, 1-\BaseAlpha] = [0.5, 0.7]$:

The SR best response now depends on the revealed state:

\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{State $\theta$} & \textbf{$\pi(\theta)$} & \textbf{SR Belief $F(G|\theta)$} & \textbf{SR Action} & \textbf{LR Payoff} \\
\midrule
$G$ & \BasePiG & $\SRBeliefAfterG > \SRThreshold$ & Cooperate & $u_1(G, A, C)$ \\
$B$ & \BasePiB & $\SRBeliefAfterB < \SRThreshold$ & Defect & $u_1(B, F, D)$ \\
\bottomrule
\end{tabular}
\caption{State-contingent SR behavior in the non-belief-robust deterrence game. SR cooperates in good states (where $F(G|G) = \SRBeliefAfterG > \mu^* = \SRThreshold$) but defects in bad states (where $F(G|B) = \SRBeliefAfterB < \mu^*$).}
\label{tab:sr_behavior}
\end{table}

By Theorem~\ref{thm:general}, the corrected bound is:
\[
    V_{\mathrm{Markov}} = \pi(G) \cdot u_1(G, A, C) + \pi(B) \cdot u_1(B, F, D) = \PayoffFiltered.
\]

\subsection{The Overestimation Gap}

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Scenario} & \textbf{LR Average Payoff} & \textbf{Assumption} \\
\midrule
Stationary beliefs (original paper) & \PayoffStationary & $\mu = \pi(G)$ always \\
Filtered beliefs (reality) & \PayoffFiltered & $\mu = F(G|\theta_t)$ \\
\midrule
\textbf{Overestimation} & \textbf{\PayoffOverestimation\%} & \\
\bottomrule
\end{tabular}
\end{center}

The overestimation arises because the original analysis assumes SR always faces belief $\pi(G) = \BasePiG > \SRThreshold$, so SR always cooperates. In reality, SR defects in bad states (where $F(G|B) = \SRBeliefAfterB < \SRThreshold$), reducing the LR payoff by \PayoffOverestimation\%.

\subsection{Comparison Table}

\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Quantity} & \textbf{i.i.d.} & \textbf{Markov (belief-robust)} & \textbf{Markov (general)} \\
\midrule
SR belief about $\theta_{t+1}$ & $\pi$ & $\pi$ & $F(\cdot|\theta_t)$ \\
SR behavior & Static & Static & State-contingent \\
Commitment payoff & $V(s_1^*)$ & $V(s_1^*)$ & $V_{\mathrm{Markov}} \leq V(s_1^*)$ \\
Gap from i.i.d. & 0 & 0 & $\BeliefGapFormula$ \\
\bottomrule
\end{tabular}
\caption{Summary of the three regimes for the deterrence game.}
\label{tab:comparison}
\end{table}

\subsection{Figures}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig_payoff_gap.png}
\caption{LR payoff comparison: stationary belief assumption gives \PayoffStationary{} vs.\ filtered belief reality of \PayoffFiltered, a \PayoffOverestimation\% overestimation. The gap is entirely explained by SR defection in bad states.}
\label{fig:payoff_gap}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig_nash_dynamics.png}
\caption{Belief trajectory crossing the BR threshold $\mu^* = \SRThreshold$. The SR player's belief $F(G|\theta_t)$ oscillates between \SRBeliefAfterG{} (after $G$) and \SRBeliefAfterB{} (after $B$), crossing $\mu^*$ with each state transition. Disagreement rate: \SRDisagreement\%.}
\label{fig:nash_dynamics}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig_belief_gap.png}
\caption{Analytical belief gap $2\alpha\beta|1-\alpha-\beta|/(\alpha+\beta)^2$ across the $(\alpha,\beta)$ parameter space. The gap equals zero along the anti-diagonal $\alpha+\beta=1$ (i.i.d.\ line) and increases with persistence $|1-\alpha-\beta|$.}
\label{fig:belief_gap}
\end{figure}
