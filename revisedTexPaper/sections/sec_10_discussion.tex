% ================================================================
\section{Discussion and Open Questions}\label{sec:discussion}

\subsection{Summary}

We have shown that extending Marginal Reputation to Markov states is more subtle than initially claimed, requiring a distinction between two regimes. In belief-robust games---where the short-run player's best-response set does not depend on the revealed state---the i.i.d.\ commitment payoff bound $V(s_1^*)$ holds exactly (Theorem~\ref{thm:belief_robust}). In general games, the corrected Markov commitment payoff $V_{\mathrm{Markov}}(s_1^*) \leq V(s_1^*)$ provides the appropriate bound (Theorem~\ref{thm:general}). The gap between the two, $V(s_1^*) - V_{\mathrm{Markov}}$, is a new economic object---the cost of persistence---quantifying how state persistence affects reputation-building by enabling the short-run player to condition behavior on the revealed state.

\subsection{Open Questions}

Several directions merit further investigation.

The \textbf{belief-robustness landscape} remains incompletely characterized. For the deterrence game, Proposition~\ref{prop:br_condition} gives a clean criterion in terms of the SR threshold and the filtering beliefs. For general games with richer action spaces, the geometry of the belief-robustness condition may be more complex. An important question is whether belief-robustness is generic or exceptional within economically relevant classes of games.

The \textbf{computation of $V_{\mathrm{Markov}}$} is straightforward for the two-state deterrence game but may be challenging for general supermodular games, where it requires solving state-contingent Nash equilibria for each $\theta \in \Theta$ and integrating over the ergodic distribution. Closed-form expressions or tight bounds for broad classes of games would make Theorem~\ref{thm:general} more practically useful.

A natural question concerns \textbf{$\varepsilon$-perturbed strategies}. If the commitment type plays $s_1^\varepsilon(\theta) = (1-\varepsilon) s_1^*(\theta) + \varepsilon \cdot \text{uniform}$ for small $\varepsilon > 0$, the strategy is no longer state-revealing, and filter stability (SA4) suggests that beliefs may converge to the stationary distribution. Whether $V_{\mathrm{Markov}}(s_1^\varepsilon) \to V(s_1^*)$ as $\varepsilon \to 0$, uniformly in other parameters, would provide a ``smoothing'' route to the full bound that circumvents the belief-robustness requirement.

The \textbf{rate of convergence}---how fast $\underline{U}_1(\delta) \to V_{\mathrm{Markov}}$ as $\delta \to 1$---is not addressed by our analysis. The rate likely depends on both the mixing time $\tau_{\mathrm{mix}}$ and the belief-robustness margin $\min_\theta |F(G|\theta) - \mu^*|$, and characterizing this dependence would be valuable for applications.

Extensions to \textbf{continuous state spaces}, where $\Theta$ is infinite (e.g., $\R$), would require the OT problem to be formulated in infinite dimensions. The result should extend under compactness and continuity conditions, but care is needed with the cyclical monotonicity characterization.

Finally, the case of \textbf{non-revealing strategies}---commitment strategies with full support on $A_1$ for all $\theta$, so that the signal does not perfectly identify the state---deserves separate treatment. For such strategies, filter stability suggests that the belief dynamics may be more benign than in the state-revealing case, and it is plausible that the full bound $V(s_1^*)$ is recoverable without the belief-robustness condition. A related notion of \textbf{approximate belief-robustness}, defined as $\sup_{\theta,\theta'} d_H(B(s_1^*, F(\cdot|\theta)), B(s_1^*, F(\cdot|\theta'))) \leq \varepsilon$, may yield a bound of the form $V_{\mathrm{Markov}} \geq V(s_1^*) - C\varepsilon$ for some constant $C$.

\subsection{Conclusion}

Persistence in states creates a fundamental tension between the long-run player's reputation-building and the short-run player's state-learning. When the Stackelberg strategy reveals the state, short-run players learn the state sequence and adjust their behavior accordingly, reducing the long-run player's commitment payoff by exactly the amount of behavioral adjustment. This tension---invisible in the i.i.d.\ framework and quantified here for the first time---is a genuinely new economic insight that enriches the marginal reputation framework. The concepts of belief-robustness and the Markov commitment payoff provide the tools to analyze reputation in dynamic environments where states exhibit persistence, answering the open question posed by Luo \& Wolitzky (2024, footnote~9).
